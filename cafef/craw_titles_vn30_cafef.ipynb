{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Volumes/MTS800/tmp/hust/study_at_hust/semesters/2025_1/DS/hello_ngoc_linh/cafef\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.14.2)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.32.5)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.2.6)\n",
            "Requirement already satisfied: openpyxl in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (3.1.5)\n",
            "Requirement already satisfied: openai in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.8.1)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (1.2.1)\n",
            "Requirement already satisfied: selenium in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.38.0)\n",
            "Requirement already satisfied: webdriver-manager in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.0.2)\n",
            "Requirement already satisfied: httpx in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (0.28.1)\n",
            "Requirement already satisfied: pydantic in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.12.4)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et_xmlfile in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: trio<1.0,>=0.31.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.32.0)\n",
            "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et_xmlfile in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: trio<1.0,>=0.31.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.32.0)\n",
            "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests pandas numpy openpyxl openai python-dotenv selenium webdriver-manager httpx pydantic tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Cbz0vQUk8Q0l"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "LIMIT_CATEGORY = True\n",
        "NUM_CATEGORIES = 5\n",
        "MAX_PAGES_PER_CATEGORY = 5\n",
        "\n",
        "# Date filtering option\n",
        "FILTER_BY_DATE = False  # Set to False to disable date filtering\n",
        "\n",
        "# Date range filter (inclusive)\n",
        "START_DATE = datetime(2020, 1, 1)\n",
        "END_DATE = datetime(2024, 12, 31)\n",
        "\n",
        "# Base URL for cafef website\n",
        "BASE_URL = \"https://cafef.vn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key loaded successfully\n",
            "Found 26 categories\n",
            "Found 26 categories\n",
            "Failed to parse JSON. Here's raw output:\n",
            "```json\n",
            "[\n",
            "    {\n",
            "        \"title\": \"XÃ HỘI\",\n",
            "        \"url\": \"https://cafef.vn/xa-hoi.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"CHỨNG KHOÁN\",\n",
            "        \"url\": \"https://cafef.vn/thi-truong-chung-khoan.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"BẤT ĐỘNG SẢN\",\n",
            "        \"url\": \"https://cafef.vn/bat-dong-san.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"DOANH NGHIỆP\",\n",
            "        \"url\": \"https://cafef.vn/doanh-nghiep.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"NGÂN HÀNG\",\n",
            "        \"url\": \"https://cafef.vn/tai-chinh-ngan-hang.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Smart Money\",\n",
            "        \"url\": \"https://cafef.vn/smart-money.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"TÀI CHÍNH QUỐC TẾ\",\n",
            "        \"url\": \"https://cafef.vn/tai-chinh-quoc-te.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"VĨ MÔ\",\n",
            "        \"url\": \"https://cafef.vn/vi-mo-dau-tu.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"KINH TẾ SỐ\",\n",
            "        \"url\": \"https://cafef.vn/kinh-te-so.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"THỊ TRƯỜNG\",\n",
            "        \"url\": \"https://cafef.vn/thi-truong.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"SỐNG\",\n",
            "        \"url\": \"https://cafef.vn/song.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"LIFESTYLE\",\n",
            "        \"url\": \"https://cafef.vn/lifestyle.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Dữ liệu\",\n",
            "        \"url\": \"https://cafef.vn/du-lieu.chn\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "WARNING: No filtered categories returned. Using all categories instead.\n",
            "\n",
            "Filtered Categories (26):\n",
            "- XÃ HỘI: https://cafef.vn/xa-hoi.chn\n",
            "- CHỨNG KHOÁN: https://cafef.vn/thi-truong-chung-khoan.chn\n",
            "- BẤT ĐỘNG SẢN: https://cafef.vn/bat-dong-san.chn\n",
            "- DOANH NGHIỆP: https://cafef.vn/doanh-nghiep.chn\n",
            "- NGÂN HÀNG: https://cafef.vn/tai-chinh-ngan-hang.chn\n",
            "- Smart Money: https://cafef.vn/smart-money.chn\n",
            "- TÀI CHÍNH QUỐC TẾ: https://cafef.vn/tai-chinh-quoc-te.chn\n",
            "- VĨ MÔ: https://cafef.vn/vi-mo-dau-tu.chn\n",
            "- KINH TẾ SỐ: https://cafef.vn/kinh-te-so.chn\n",
            "- THỊ TRƯỜNG: https://cafef.vn/thi-truong.chn\n",
            "- SỐNG: https://cafef.vn/song.chn\n",
            "- LIFESTYLE: https://cafef.vn/lifestyle.chn\n",
            "- xã hội: https://cafef.vn/xa-hoi.chn\n",
            "- doanh nghiệp: https://cafef.vn/doanh-nghiep.chn\n",
            "- kinh tế vĩ mô: https://cafef.vn/vi-mo-dau-tu.chn\n",
            "- chứng khoáng: https://cafef.vn/thi-truong-chung-khoan.chn\n",
            "- tài chính ngân hàng: https://cafef.vn/tai-chinh-ngan-hang.chn\n",
            "- tài chính quốc tế: https://cafef.vn/tai-chinh-quoc-te.chn\n",
            "- tin tức: https://cafef.vn/bat-dong-san.chn\n",
            "- dự án: https://cafef.vn/du-an.chn\n",
            "- bản đồ dự án: https://cafef.vn/ban-do-du-an.chn\n",
            "- hàng hóa nguyên liệu: https://cafef.vn/hang-hoa-nguyen-lieu.chn\n",
            "- sống: https://cafef.vn/song.chn\n",
            "- Lifestyle: https://cafef.vn/lifestyle.chn\n",
            "- Magazine: https://cafef.vn/nhom-chu-de/emagazine.chn\n",
            "- Dữ liệu: https://cafef.vn/du-lieu.chn\n",
            "Failed to parse JSON. Here's raw output:\n",
            "```json\n",
            "[\n",
            "    {\n",
            "        \"title\": \"XÃ HỘI\",\n",
            "        \"url\": \"https://cafef.vn/xa-hoi.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"CHỨNG KHOÁN\",\n",
            "        \"url\": \"https://cafef.vn/thi-truong-chung-khoan.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"BẤT ĐỘNG SẢN\",\n",
            "        \"url\": \"https://cafef.vn/bat-dong-san.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"DOANH NGHIỆP\",\n",
            "        \"url\": \"https://cafef.vn/doanh-nghiep.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"NGÂN HÀNG\",\n",
            "        \"url\": \"https://cafef.vn/tai-chinh-ngan-hang.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Smart Money\",\n",
            "        \"url\": \"https://cafef.vn/smart-money.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"TÀI CHÍNH QUỐC TẾ\",\n",
            "        \"url\": \"https://cafef.vn/tai-chinh-quoc-te.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"VĨ MÔ\",\n",
            "        \"url\": \"https://cafef.vn/vi-mo-dau-tu.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"KINH TẾ SỐ\",\n",
            "        \"url\": \"https://cafef.vn/kinh-te-so.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"THỊ TRƯỜNG\",\n",
            "        \"url\": \"https://cafef.vn/thi-truong.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"SỐNG\",\n",
            "        \"url\": \"https://cafef.vn/song.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"LIFESTYLE\",\n",
            "        \"url\": \"https://cafef.vn/lifestyle.chn\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Dữ liệu\",\n",
            "        \"url\": \"https://cafef.vn/du-lieu.chn\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "WARNING: No filtered categories returned. Using all categories instead.\n",
            "\n",
            "Filtered Categories (26):\n",
            "- XÃ HỘI: https://cafef.vn/xa-hoi.chn\n",
            "- CHỨNG KHOÁN: https://cafef.vn/thi-truong-chung-khoan.chn\n",
            "- BẤT ĐỘNG SẢN: https://cafef.vn/bat-dong-san.chn\n",
            "- DOANH NGHIỆP: https://cafef.vn/doanh-nghiep.chn\n",
            "- NGÂN HÀNG: https://cafef.vn/tai-chinh-ngan-hang.chn\n",
            "- Smart Money: https://cafef.vn/smart-money.chn\n",
            "- TÀI CHÍNH QUỐC TẾ: https://cafef.vn/tai-chinh-quoc-te.chn\n",
            "- VĨ MÔ: https://cafef.vn/vi-mo-dau-tu.chn\n",
            "- KINH TẾ SỐ: https://cafef.vn/kinh-te-so.chn\n",
            "- THỊ TRƯỜNG: https://cafef.vn/thi-truong.chn\n",
            "- SỐNG: https://cafef.vn/song.chn\n",
            "- LIFESTYLE: https://cafef.vn/lifestyle.chn\n",
            "- xã hội: https://cafef.vn/xa-hoi.chn\n",
            "- doanh nghiệp: https://cafef.vn/doanh-nghiep.chn\n",
            "- kinh tế vĩ mô: https://cafef.vn/vi-mo-dau-tu.chn\n",
            "- chứng khoáng: https://cafef.vn/thi-truong-chung-khoan.chn\n",
            "- tài chính ngân hàng: https://cafef.vn/tai-chinh-ngan-hang.chn\n",
            "- tài chính quốc tế: https://cafef.vn/tai-chinh-quoc-te.chn\n",
            "- tin tức: https://cafef.vn/bat-dong-san.chn\n",
            "- dự án: https://cafef.vn/du-an.chn\n",
            "- bản đồ dự án: https://cafef.vn/ban-do-du-an.chn\n",
            "- hàng hóa nguyên liệu: https://cafef.vn/hang-hoa-nguyen-lieu.chn\n",
            "- sống: https://cafef.vn/song.chn\n",
            "- Lifestyle: https://cafef.vn/lifestyle.chn\n",
            "- Magazine: https://cafef.vn/nhom-chu-de/emagazine.chn\n",
            "- Dữ liệu: https://cafef.vn/du-lieu.chn\n"
          ]
        }
      ],
      "source": [
        "# Check if API key is loaded\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"WARNING: OPENAI_API_KEY not found in environment variables!\")\n",
        "    print(\"Please create a .env file with: OPENAI_API_KEY=your-api-key\")\n",
        "    print(\"Or set it manually below:\")\n",
        "    # Uncomment and add your key here if .env doesn't work\n",
        "    # api_key = \"your-api-key-here\"\n",
        "else:\n",
        "    print(\"API key loaded successfully\")\n",
        "\n",
        "def get_categories():\n",
        "    # Fetch the homepage HTML\n",
        "    html = requests.get(BASE_URL, timeout=10).text\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    categories = []\n",
        "\n",
        "    # Find the menu category div\n",
        "    menu_div = soup.find(\"div\", class_=\"menucategory\")\n",
        "    if menu_div:\n",
        "        # Find all anchor tags within the menu\n",
        "        for a in menu_div.find_all(\"a\", href=True):\n",
        "            href = a.get(\"href\", \"\")\n",
        "            title = a.get(\"title\", a.text).strip()\n",
        "            \n",
        "            # Skip empty titles and home page\n",
        "            if not title or href == \"/\":\n",
        "                continue\n",
        "            \n",
        "            # Only keep links ending with .chn\n",
        "            if href.endswith(\".chn\"):\n",
        "                # Normalize the URL to absolute path\n",
        "                if href.startswith(\"/\"):\n",
        "                    full_url = BASE_URL.rstrip(\"/\") + href\n",
        "                else:\n",
        "                    full_url = href\n",
        "\n",
        "                categories.append({\n",
        "                    \"title\": title,\n",
        "                    \"url\": full_url\n",
        "                })\n",
        "\n",
        "    return categories\n",
        "\n",
        "\n",
        "def filter_category_pages(categories):\n",
        "    # Initialize OpenAI client with API key from environment variable\n",
        "    if not api_key:\n",
        "        print(\"Skipping GPT filtering - using all categories\")\n",
        "        return categories\n",
        "    \n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Format categories as text for GPT\n",
        "    urls_text = \"\\n\".join([f\"- {c['title']}: {c['url']}\" for c in categories])\n",
        "\n",
        "    # Call GPT to filter out article pages and keep only main category pages\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an assistant that filters a list of URLs. \"\n",
        "                           \"Keep only main category pages and remove specific news articles or subcategories. \"\n",
        "                           \"Return only a JSON array of objects with 'title' and 'url'.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Here is the list:\\n{urls_text}\\n\\nFilter them and return JSON array.\"\n",
        "            }\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Parse the JSON response\n",
        "    filtered_json = response.choices[0].message.content\n",
        "    try:\n",
        "        filtered_list = json.loads(filtered_json)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Failed to parse JSON. Here's raw output:\")\n",
        "        print(filtered_json)\n",
        "        filtered_list = []\n",
        "\n",
        "    return filtered_list\n",
        "\n",
        "\n",
        "# Get all categories from the homepage\n",
        "cats = get_categories()\n",
        "\n",
        "# Check if categories were found\n",
        "if not cats:\n",
        "    print(\"ERROR: No categories found!\")\n",
        "else:\n",
        "    print(f\"Found {len(cats)} categories\")\n",
        "\n",
        "# Filter to keep only main category pages\n",
        "filtered_cats = filter_category_pages(cats)\n",
        "\n",
        "# Check if filtering returned results\n",
        "if not filtered_cats:\n",
        "    print(\"WARNING: No filtered categories returned. Using all categories instead.\")\n",
        "    filtered_cats = cats\n",
        "\n",
        "# Display filtered categories\n",
        "print(f\"\\nFiltered Categories ({len(filtered_cats)}):\")\n",
        "for c in filtered_cats:\n",
        "    print(f\"- {c['title']}: {c['url']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hardcoded zone_id for all categories\n",
        "DEFAULT_ZONE_ID = 18832\n",
        "\n",
        "# Mapping of VN30 stock codes to their related keywords\n",
        "KEYWORDS_MAP = {\n",
        "    \"ACB\": [\"ACB\", \"Ngân hàng ACB\", \"Ngân hàng TMCP Á Châu\"],\n",
        "    \"BCM\": [\"BCM\", \"Becamex\", \"KCN Bình Dương\", \"khu công nghiệp Bình Dương\", \"VSIP\", \"Becamex IDC\"],\n",
        "    \"BID\": [\"BIDV\", \"Ngân hàng Đầu tư và Phát triển Việt Nam\"],\n",
        "    \"CTG\": [\"CTG\", \"VietinBank\", \"Ngân hàng Công Thương Việt Nam\"],\n",
        "    \"DGC\": [\"DGC\", \"Hóa chất Đức Giang\"],\n",
        "    \"FPT\": [\"FPT\"],\n",
        "    \"GAS\": [\"PV GAS\", \"PV Gas\", \"Tổng Công ty Khí Việt Nam\"],\n",
        "    \"GVR\": [\"GVR\", \"Tập đoàn Cao su\", \"Tập đoàn Công nghiệp Cao su Việt Nam\"],\n",
        "    \"HDB\": [\"HDB\", \"HDBank\", \"Ngân hàng TMCP Phát triển Thành phố Hồ Chí Minh\"],\n",
        "    \"HPG\": [\"HPG\", \"Hòa Phát\"],\n",
        "    \"LPB\": [\"LPB\", \"LPBank\", \"LienVietPostBank\", \"Ngân hàng Bưu điện Liên Việt\"],\n",
        "    \"MBB\": [\"MBB\", \"MBBank\", \"Ngân hàng Quân đội\", \"MB\", \"Ngân hàng TMCP Quân đội\"],\n",
        "    \"MSN\": [\"MSN\", \"Masan\", \"WinCommerce\"],\n",
        "    \"MWG\": [\"MWG\", \"Thế Giới Di Động\", \"Mobile World\", \"Bách Hóa Xanh\", \"BHX\", \"Điện Máy Xanh\", \"ĐMX\", \"TGDĐ\"],\n",
        "    \"PLX\": [\"PLX\", \"Petrolimex\", \"Tập đoàn Xăng dầu Việt Nam\"],\n",
        "    \"SAB\": [\"SAB\", \"Sabeco\", \"Tổng Công ty CP Bia - Rượu - Nước giải khát Sài Gòn\"],\n",
        "    \"SHB\": [\"SHB\", \"Ngân hàng Thương mại Cổ phần Sài Gòn – Hà Nội\", \"Ngân hàng TMCP Sài Gòn Hà Nội\"],\n",
        "    \"SSB\": [\"SSB\", \"Ngân hàng Thương mại Cổ phần Đông Nam Á\", \"Ngân hàng TMCP Đông Nam Á\", \"SeABank\"],\n",
        "    \"SSI\": [\"SSI\", \"Chứng khoán SSI\"],\n",
        "    \"STB\": [\"STB\", \"Sài Gòn Thương Tín\", \"Sacombank\"],\n",
        "    \"TCB\": [\"TCB\", \"Techcombank\", \"Ngân hàng TMCP Kỹ Thương Việt Nam\"],\n",
        "    \"TPB\": [\"TPB\", \"TPBank\", \"Ngân hàng Tiên Phong\", \"Ngân hàng TMCP Tiên Phong\"],\n",
        "    \"VCB\": [\"VCB\", \"Vietcombank\", \"Ngân hàng TMCP Ngoại Thương Việt Nam\", \"Ngân hàng Ngoại thương\"],\n",
        "    \"VHM\": [\"VHM\", \"Vinhomes\"],\n",
        "    \"VIB\": [\"VIB\", \"Ngân hàng TMCP Quốc Tế Việt Nam\", \"Ngân hàng Quốc Tế\"],\n",
        "    \"VIC\": [\"VIC\", \"Vingroup\", \"Công ty Cổ phần Tập đoàn Vingroup\"],\n",
        "    \"VJC\": [\"VJC\", \"Vietjet Air\", \"Công ty Cổ phần Hàng không Vietjet\", \"máy bay Vietjet\"],\n",
        "    \"VNM\": [\"VNM\", \"Vinamilk\", \"Công ty Cổ phần Sữa Việt Nam\"],\n",
        "    \"VPB\": [\"VPB\", \"VPBank\", \"Ngân hàng TMCP Việt Nam Thịnh Vượng\"],\n",
        "    \"VRE\": [\"VRE\", \"Vincom Retail\", \"Công ty Cổ phần Vincom Retail\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "20quBazH8U5M"
      },
      "outputs": [],
      "source": [
        "class CafefScraper:\n",
        "    BASE_URL = \"https://cafef.vn\"\n",
        "    \n",
        "    def __init__(self, zone_id, category_url, category_name=\"\"):\n",
        "        self.zone_id = zone_id\n",
        "        self.category_url = category_url\n",
        "        self.category_name = category_name\n",
        "        self.headers = {\n",
        "            'Accept': '*/*',\n",
        "            'Referer': category_url,\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'X-Requested-With': 'XMLHttpRequest',\n",
        "        }\n",
        "        self.articles = []\n",
        "    \n",
        "    def parse_html(self, html_content):\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        articles = []\n",
        "        \n",
        "        article_elements = soup.select('.firstitem, .cate-hl-row2 .big, .tlitem.box-category-item')\n",
        "        \n",
        "        for article in article_elements:\n",
        "            try:\n",
        "                title_element = article.find(['h2', 'h3']).find('a')\n",
        "                if not title_element:\n",
        "                    continue\n",
        "                \n",
        "                title = title_element.get('title', title_element.text).strip()\n",
        "                relative_link = title_element['href']\n",
        "                link = self.BASE_URL + relative_link if relative_link.startswith('/') else relative_link\n",
        "                \n",
        "                summary_element = article.find('p', class_='sapo')\n",
        "                summary = summary_element.text.strip() if summary_element else \"N/A\"\n",
        "                \n",
        "                articles.append({\n",
        "                    'Title': title,\n",
        "                    'Link': link,\n",
        "                    'Summary': summary,\n",
        "                })\n",
        "            except Exception:\n",
        "                continue\n",
        "        \n",
        "        return articles\n",
        "    \n",
        "    def scrape_initial_page(self):\n",
        "        print(f\"Loading initial page: {self.category_url}\")\n",
        "        try:\n",
        "            response = requests.get(self.category_url, headers=self.headers, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            initial_articles = self.parse_html(response.text)\n",
        "            self.articles.extend(initial_articles)\n",
        "            print(f\"Found {len(initial_articles)} articles on initial page\")\n",
        "            return True\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error loading initial page: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def scrape_api_pages(self, MAX_PAGES_PER_CATEGORY):\n",
        "        for page_num in range(1, MAX_PAGES_PER_CATEGORY + 1):\n",
        "            api_url = f\"{self.BASE_URL}/timelinelist/{self.zone_id}/{page_num}.chn\"\n",
        "            print(f\"Loading API page {page_num}...\")\n",
        "            \n",
        "            try:\n",
        "                response = requests.get(api_url, headers=self.headers, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                \n",
        "                if not response.text.strip():\n",
        "                    print(\"  ⚠ No more articles available. Reached end of pages.\")\n",
        "                    break\n",
        "                \n",
        "                page_articles = self.parse_html(response.text)\n",
        "                \n",
        "                if not page_articles:\n",
        "                    print(\"  ⚠ Last page reached (no articles found).\")\n",
        "                    break\n",
        "                \n",
        "                self.articles.extend(page_articles)\n",
        "                print(f\"Loaded {len(page_articles)} articles from page {page_num}\")\n",
        "                \n",
        "                time.sleep(1)\n",
        "                \n",
        "            except requests.RequestException as e:\n",
        "                print(f\"Error loading API page {page_num}: {e}\")\n",
        "                break\n",
        "    \n",
        "    def scrape(self, max_pages=5):\n",
        "        if not self.scrape_initial_page():\n",
        "            return []\n",
        "        \n",
        "        self.scrape_api_pages(MAX_PAGES_PER_CATEGORY)\n",
        "        return self.articles\n",
        "    \n",
        "    def to_dataframe(self):\n",
        "        df = pd.DataFrame(self.articles)\n",
        "        df.drop_duplicates(subset=['Link'], inplace=True, keep='first')\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fiylw8xQ8Ou_",
        "outputId": "ea229578-754b-4232-e451-92444a7c7d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting to scrape 26 categories\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "[1/26] Processing category: XÃ HỘI\n",
            "URL: https://cafef.vn/xa-hoi.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/xa-hoi.chn\n",
            "Found 24 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Found 24 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Total unique articles found: 84\n",
            "✓ Category 'XÃ HỘI' completed (1/26)\n",
            "Total unique articles found: 84\n",
            "✓ Category 'XÃ HỘI' completed (1/26)\n",
            "\n",
            "============================================================\n",
            "[2/26] Processing category: CHỨNG KHOÁN\n",
            "URL: https://cafef.vn/thi-truong-chung-khoan.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/thi-truong-chung-khoan.chn\n",
            "\n",
            "============================================================\n",
            "[2/26] Processing category: CHỨNG KHOÁN\n",
            "URL: https://cafef.vn/thi-truong-chung-khoan.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/thi-truong-chung-khoan.chn\n",
            "Found 23 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Found 23 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 5...\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Loaded 15 articles from page 5\n",
            "Total unique articles found: 83\n",
            "✓ Category 'CHỨNG KHOÁN' completed (2/26)\n",
            "Total unique articles found: 83\n",
            "✓ Category 'CHỨNG KHOÁN' completed (2/26)\n",
            "\n",
            "============================================================\n",
            "[3/26] Processing category: BẤT ĐỘNG SẢN\n",
            "URL: https://cafef.vn/bat-dong-san.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/bat-dong-san.chn\n",
            "\n",
            "============================================================\n",
            "[3/26] Processing category: BẤT ĐỘNG SẢN\n",
            "URL: https://cafef.vn/bat-dong-san.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/bat-dong-san.chn\n",
            "Found 23 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Found 23 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Total unique articles found: 83\n",
            "✓ Category 'BẤT ĐỘNG SẢN' completed (3/26)\n",
            "Total unique articles found: 83\n",
            "✓ Category 'BẤT ĐỘNG SẢN' completed (3/26)\n",
            "\n",
            "============================================================\n",
            "[4/26] Processing category: DOANH NGHIỆP\n",
            "URL: https://cafef.vn/doanh-nghiep.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/doanh-nghiep.chn\n",
            "\n",
            "============================================================\n",
            "[4/26] Processing category: DOANH NGHIỆP\n",
            "URL: https://cafef.vn/doanh-nghiep.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/doanh-nghiep.chn\n",
            "Found 23 articles on initial page\n",
            "Loading API page 1...\n",
            "Found 23 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Loaded 15 articles from page 1\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Total unique articles found: 90\n",
            "✓ Category 'DOANH NGHIỆP' completed (4/26)\n",
            "Total unique articles found: 90\n",
            "✓ Category 'DOANH NGHIỆP' completed (4/26)\n",
            "\n",
            "============================================================\n",
            "[5/26] Processing category: NGÂN HÀNG\n",
            "URL: https://cafef.vn/tai-chinh-ngan-hang.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/tai-chinh-ngan-hang.chn\n",
            "\n",
            "============================================================\n",
            "[5/26] Processing category: NGÂN HÀNG\n",
            "URL: https://cafef.vn/tai-chinh-ngan-hang.chn\n",
            "============================================================\n",
            "Zone ID: 18832\n",
            "Loading initial page: https://cafef.vn/tai-chinh-ngan-hang.chn\n",
            "Found 23 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Found 23 articles on initial page\n",
            "Loading API page 1...\n",
            "Loaded 15 articles from page 1\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 2...\n",
            "Loaded 15 articles from page 2\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 3...\n",
            "Loaded 15 articles from page 3\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 4...\n",
            "Loaded 15 articles from page 4\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Loading API page 5...\n",
            "Loaded 15 articles from page 5\n",
            "Total unique articles found: 83\n",
            "✓ Category 'NGÂN HÀNG' completed (5/26)\n",
            "Total unique articles found: 83\n",
            "✓ Category 'NGÂN HÀNG' completed (5/26)\n",
            "\n",
            "============================================================\n",
            "Scraping completed for all categories\n",
            "Total articles collected: 423\n",
            "\n",
            "============================================================\n",
            "Scraping completed for all categories\n",
            "Total articles collected: 423\n"
          ]
        }
      ],
      "source": [
        "# Check if we have categories to scrape\n",
        "if not filtered_cats:\n",
        "    print(\"ERROR: No categories available to scrape!\")\n",
        "else:\n",
        "    # Store all articles from all categories\n",
        "    all_articles_list = []\n",
        "    \n",
        "    total_categories = len(filtered_cats)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting to scrape {total_categories} categories\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Scrape each category\n",
        "    cnt = 0\n",
        "    for idx, cat in enumerate(filtered_cats, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[{idx}/{total_categories}] Processing category: {cat['title']}\")\n",
        "        print(f\"URL: {cat['url']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Use hardcoded zone_id\n",
        "        zone_id = DEFAULT_ZONE_ID\n",
        "        print(f\"Zone ID: {zone_id}\")\n",
        "        \n",
        "        # Create scraper instance\n",
        "        scraper = CafefScraper(zone_id, cat['url'], cat['title'])\n",
        "        scraper.scrape(max_pages=MAX_PAGES_PER_CATEGORY)\n",
        "        \n",
        "        # Convert to dataframe\n",
        "        df = scraper.to_dataframe()\n",
        "        \n",
        "        print(f\"Total unique articles found: {len(df)}\")\n",
        "        \n",
        "        # Add articles to list\n",
        "        all_articles_list.extend(df.to_dict('records'))\n",
        "        \n",
        "        print(f\"✓ Category '{cat['title']}' completed ({idx}/{total_categories})\")\n",
        "    \n",
        "        # Delay between categories\n",
        "        time.sleep(2)\n",
        "        \n",
        "        if LIMIT_CATEGORY:\n",
        "            cnt += 1\n",
        "            if cnt >= NUM_CATEGORIES:\n",
        "                break\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping completed for all categories\")\n",
        "    print(f\"Total articles collected: {len(all_articles_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Filtering and grouping articles by VN30 stock codes...\n",
            "============================================================\n",
            "\n",
            "Total articles before filtering: 423\n",
            "Articles matching VN30 keywords: 19\n",
            "\n",
            "============================================================\n",
            "Extracting publication dates from article pages...\n",
            "============================================================\n",
            "\n",
            "[1/19] Extracting date for: Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vie...\n",
            "Articles matching VN30 keywords: 19\n",
            "\n",
            "============================================================\n",
            "Extracting publication dates from article pages...\n",
            "============================================================\n",
            "\n",
            "[1/19] Extracting date for: Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vie...\n",
            "[2/19] Extracting date for: Động thái lạ của các “ông lớn” ngân hàng giữa làn ...\n",
            "[2/19] Extracting date for: Động thái lạ của các “ông lớn” ngân hàng giữa làn ...\n",
            "[3/19] Extracting date for: Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vie...\n",
            "[3/19] Extracting date for: Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vie...\n",
            "[4/19] Extracting date for: Tỷ phú Trần Đình Long và dàn lãnh đạo Hòa Phát đem...\n",
            "[4/19] Extracting date for: Tỷ phú Trần Đình Long và dàn lãnh đạo Hòa Phát đem...\n",
            "[5/19] Extracting date for: UBCKNN “bật đèn xanh”, SSI sắp tăng vốn điều lệ vư...\n",
            "[5/19] Extracting date for: UBCKNN “bật đèn xanh”, SSI sắp tăng vốn điều lệ vư...\n",
            "[6/19] Extracting date for: Techcombank tăng lãi suất tiết kiệm lên kịch trần...\n",
            "[6/19] Extracting date for: Techcombank tăng lãi suất tiết kiệm lên kịch trần...\n",
            "[7/19] Extracting date for: 8 ngân hàng tăng lãi suất tiết kiệm tuần qua: Nhiề...\n",
            "[7/19] Extracting date for: 8 ngân hàng tăng lãi suất tiết kiệm tuần qua: Nhiề...\n",
            "[8/19] Extracting date for: Một ngân hàng thông báo khẩn về tình trạng gián đo...\n",
            "[8/19] Extracting date for: Một ngân hàng thông báo khẩn về tình trạng gián đo...\n",
            "[9/19] Extracting date for: 8 ngân hàng tăng lãi suất tiết kiệm tuần qua: Nhiề...\n",
            "[9/19] Extracting date for: 8 ngân hàng tăng lãi suất tiết kiệm tuần qua: Nhiề...\n",
            "[10/19] Extracting date for: Thêm 3 ngân hàng vừa tăng lãi suất gửi tiết kiệm...\n",
            "[10/19] Extracting date for: Thêm 3 ngân hàng vừa tăng lãi suất gửi tiết kiệm...\n",
            "[11/19] Extracting date for: Động thái lạ của các “ông lớn” ngân hàng giữa làn ...\n",
            "[11/19] Extracting date for: Động thái lạ của các “ông lớn” ngân hàng giữa làn ...\n",
            "[12/19] Extracting date for: Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vie...\n",
            "[12/19] Extracting date for: Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vie...\n",
            "[13/19] Extracting date for: Toàn cảnh khu đất gần các đại dự án của Vinhomes, ...\n",
            "[13/19] Extracting date for: Toàn cảnh khu đất gần các đại dự án của Vinhomes, ...\n",
            "[14/19] Extracting date for: Diễn biến mới tại khu đô thị sân golf do công ty c...\n",
            "[14/19] Extracting date for: Diễn biến mới tại khu đô thị sân golf do công ty c...\n",
            "[15/19] Extracting date for: Khách hàng chờ thời khắc khu trung tâm của siêu đô...\n",
            "[15/19] Extracting date for: Khách hàng chờ thời khắc khu trung tâm của siêu đô...\n",
            "[16/19] Extracting date for: Người thân thành viên HĐQT VIB muốn chuyển nhượng ...\n",
            "[16/19] Extracting date for: Người thân thành viên HĐQT VIB muốn chuyển nhượng ...\n",
            "[17/19] Extracting date for: 8 ngân hàng tăng lãi suất tiết kiệm tuần qua: Nhiề...\n",
            "[17/19] Extracting date for: 8 ngân hàng tăng lãi suất tiết kiệm tuần qua: Nhiề...\n",
            "[18/19] Extracting date for: Thêm 3 ngân hàng vừa tăng lãi suất gửi tiết kiệm...\n",
            "[18/19] Extracting date for: Thêm 3 ngân hàng vừa tăng lãi suất gửi tiết kiệm...\n",
            "[19/19] Extracting date for: Vingroup và tỷ phú Phạm Nhật Vượng vừa lập kỷ lục ...\n",
            "[19/19] Extracting date for: Vingroup và tỷ phú Phạm Nhật Vượng vừa lập kỷ lục ...\n",
            "\n",
            "✓ Date extraction completed\n",
            "\n",
            "⚠ Date filtering is disabled (FILTER_BY_DATE=False)\n",
            "✓ Saved to 'titles_vn30_cafef.csv'\n",
            "\n",
            "============================================================\n",
            "Statistics by stock code (after date filtering):\n",
            "============================================================\n",
            "BID: 1 articles\n",
            "CTG: 2 articles\n",
            "HPG: 1 articles\n",
            "SSI: 1 articles\n",
            "TCB: 3 articles\n",
            "TPB: 2 articles\n",
            "VCB: 2 articles\n",
            "VHM: 3 articles\n",
            "VIB: 3 articles\n",
            "VIC: 1 articles\n",
            "\n",
            "============================================================\n",
            "Sample articles (first 5):\n",
            "============================================================\n",
            "\n",
            "[BID] 2025-11-23 - Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vietcombank kỳ hạn 12 tháng: Đồng loạt tung ưu đãi cho người gửi tiền\n",
            "Link: https://cafef.vn/lai-suat-tiet-kiem-agribank-bidv-vietinbank-vietcombank-ky-han-12-thang-dong-loat-tung-uu-dai-cho-nguoi-gui-tien-188251123110303536.chn\n",
            "\n",
            "[CTG] 2025-11-23 - Động thái lạ của các “ông lớn” ngân hàng giữa làn sóng tăng lãi suất tiền gửi\n",
            "Link: https://cafef.vn/dong-thai-la-cua-cac-ong-lon-ngan-hang-giua-lan-song-tang-lai-suat-tien-gui-188251123153710974.chn\n",
            "\n",
            "[CTG] 2025-11-23 - Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vietcombank kỳ hạn 12 tháng: Đồng loạt tung ưu đãi cho người gửi tiền\n",
            "Link: https://cafef.vn/lai-suat-tiet-kiem-agribank-bidv-vietinbank-vietcombank-ky-han-12-thang-dong-loat-tung-uu-dai-cho-nguoi-gui-tien-188251123110303536.chn\n",
            "\n",
            "[HPG] 2025-11-23 - Tỷ phú Trần Đình Long và dàn lãnh đạo Hòa Phát đem gần 60 triệu cổ phiếu HPG của cá nhân làm tài sản đảm bảo cho Nông nghiệp Hòa Phát vay\n",
            "Link: https://cafef.vn/ty-phu-tran-dinh-long-va-dan-lanh-dao-hoa-phat-dem-gan-60-trieu-co-phieu-hpg-cua-ca-nhan-lam-tai-san-dam-bao-cho-nong-nghiep-hoa-phat-vay-188251123082604359.chn\n",
            "\n",
            "[SSI] 2025-11-22 - UBCKNN “bật đèn xanh”, SSI sắp tăng vốn điều lệ vượt TCBS, lấy lại “ngôi vương” ngành chứng khoán\n",
            "Link: https://cafef.vn/ubcknn-bat-den-xanh-ssi-sap-tang-von-dieu-le-vuot-tcbs-lay-lai-ngoi-vuong-nganh-chung-khoan-188251122154746736.chn\n",
            "\n",
            "✓ Date extraction completed\n",
            "\n",
            "⚠ Date filtering is disabled (FILTER_BY_DATE=False)\n",
            "✓ Saved to 'titles_vn30_cafef.csv'\n",
            "\n",
            "============================================================\n",
            "Statistics by stock code (after date filtering):\n",
            "============================================================\n",
            "BID: 1 articles\n",
            "CTG: 2 articles\n",
            "HPG: 1 articles\n",
            "SSI: 1 articles\n",
            "TCB: 3 articles\n",
            "TPB: 2 articles\n",
            "VCB: 2 articles\n",
            "VHM: 3 articles\n",
            "VIB: 3 articles\n",
            "VIC: 1 articles\n",
            "\n",
            "============================================================\n",
            "Sample articles (first 5):\n",
            "============================================================\n",
            "\n",
            "[BID] 2025-11-23 - Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vietcombank kỳ hạn 12 tháng: Đồng loạt tung ưu đãi cho người gửi tiền\n",
            "Link: https://cafef.vn/lai-suat-tiet-kiem-agribank-bidv-vietinbank-vietcombank-ky-han-12-thang-dong-loat-tung-uu-dai-cho-nguoi-gui-tien-188251123110303536.chn\n",
            "\n",
            "[CTG] 2025-11-23 - Động thái lạ của các “ông lớn” ngân hàng giữa làn sóng tăng lãi suất tiền gửi\n",
            "Link: https://cafef.vn/dong-thai-la-cua-cac-ong-lon-ngan-hang-giua-lan-song-tang-lai-suat-tien-gui-188251123153710974.chn\n",
            "\n",
            "[CTG] 2025-11-23 - Lãi suất tiết kiệm Agribank, BIDV, VietinBank, Vietcombank kỳ hạn 12 tháng: Đồng loạt tung ưu đãi cho người gửi tiền\n",
            "Link: https://cafef.vn/lai-suat-tiet-kiem-agribank-bidv-vietinbank-vietcombank-ky-han-12-thang-dong-loat-tung-uu-dai-cho-nguoi-gui-tien-188251123110303536.chn\n",
            "\n",
            "[HPG] 2025-11-23 - Tỷ phú Trần Đình Long và dàn lãnh đạo Hòa Phát đem gần 60 triệu cổ phiếu HPG của cá nhân làm tài sản đảm bảo cho Nông nghiệp Hòa Phát vay\n",
            "Link: https://cafef.vn/ty-phu-tran-dinh-long-va-dan-lanh-dao-hoa-phat-dem-gan-60-trieu-co-phieu-hpg-cua-ca-nhan-lam-tai-san-dam-bao-cho-nong-nghiep-hoa-phat-vay-188251123082604359.chn\n",
            "\n",
            "[SSI] 2025-11-22 - UBCKNN “bật đèn xanh”, SSI sắp tăng vốn điều lệ vượt TCBS, lấy lại “ngôi vương” ngành chứng khoán\n",
            "Link: https://cafef.vn/ubcknn-bat-den-xanh-ssi-sap-tang-von-dieu-le-vuot-tcbs-lay-lai-ngoi-vuong-nganh-chung-khoan-188251122154746736.chn\n"
          ]
        }
      ],
      "source": [
        "def extract_publish_date(article_url):\n",
        "    \"\"\"\n",
        "    Extract publication date from article page's meta tag.\n",
        "    Returns date in format YYYY-MM-DD or None if not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use proper browser headers to avoid 403 errors\n",
        "        date_headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Referer': 'https://cafef.vn/',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1'\n",
        "        }\n",
        "        \n",
        "        response = requests.get(article_url, headers=date_headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        \n",
        "        # Find meta tag with article:published_time\n",
        "        meta_tag = soup.find('meta', property='article:published_time')\n",
        "        if meta_tag and meta_tag.get('content'):\n",
        "            # Extract date part (YYYY-MM-DD) from datetime string\n",
        "            datetime_str = meta_tag['content']\n",
        "            date_part = datetime_str.split('T')[0]\n",
        "            return date_part\n",
        "        \n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error extracting date: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def is_date_in_range(date_str, start_date=START_DATE, end_date=END_DATE):\n",
        "    \"\"\"\n",
        "    Check if date string is within the specified range.\n",
        "    \"\"\"\n",
        "    if not date_str or date_str == 'N/A':\n",
        "        return False\n",
        "    try:\n",
        "        article_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
        "        return start_date <= article_date <= end_date\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "def filter_and_group_by_codes(articles_list, keywords_map=KEYWORDS_MAP):\n",
        "    \"\"\"\n",
        "    Filter articles by VN30 keywords and group by stock codes.\n",
        "    Uses word boundaries to match full keywords only.\n",
        "    Returns a list of dictionaries with stock code and associated articles.\n",
        "    \"\"\"\n",
        "    # Dictionary to store articles for each code (using link as key to avoid duplicates)\n",
        "    code_articles = {code: {} for code in keywords_map.keys()}\n",
        "    \n",
        "    for article in articles_list:\n",
        "        # Combine title and summary for keyword matching\n",
        "        text = (str(article.get('Title', '')) + \" \" + str(article.get('Summary', ''))).lower()\n",
        "        matched_codes = []\n",
        "        \n",
        "        # Check each stock code's keywords\n",
        "        for code, kws in keywords_map.items():\n",
        "            for kw in kws:\n",
        "                # Use word boundary regex to match full words only\n",
        "                # \\b ensures we match complete words, not substrings\n",
        "                pattern = r'\\b' + re.escape(kw.lower()) + r'\\b'\n",
        "                if re.search(pattern, text):\n",
        "                    matched_codes.append(code)\n",
        "                    break\n",
        "        \n",
        "        # Add article to each matched code's dictionary (using link as key to prevent duplicates)\n",
        "        article_link = article.get('Link', '')\n",
        "        for code in matched_codes:\n",
        "            if article_link and article_link not in code_articles[code]:\n",
        "                code_articles[code][article_link] = {\n",
        "                    'Stock_Code': code,\n",
        "                    'Title': article.get('Title', ''),\n",
        "                    'Link': article_link,\n",
        "                    'Summary': article.get('Summary', 'N/A')\n",
        "                }\n",
        "    \n",
        "    # Flatten into single list, sorted by stock code\n",
        "    result = []\n",
        "    for code in sorted(code_articles.keys()):\n",
        "        result.extend(code_articles[code].values())\n",
        "    \n",
        "    return result, {code: list(articles.values()) for code, articles in code_articles.items()}\n",
        "\n",
        "\n",
        "# Filter and group articles by VN30 keywords\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Filtering and grouping articles by VN30 stock codes...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "if all_articles_list:\n",
        "    print(f\"Total articles before filtering: {len(all_articles_list)}\")\n",
        "    \n",
        "    # Filter and group by codes\n",
        "    grouped_articles, code_dict = filter_and_group_by_codes(all_articles_list, keywords_map=KEYWORDS_MAP)\n",
        "    \n",
        "    print(f\"Articles matching VN30 keywords: {len(grouped_articles)}\")\n",
        "    \n",
        "    # Extract publication dates for filtered articles\n",
        "    if grouped_articles:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Extracting publication dates from article pages...\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        \n",
        "        for idx, article in enumerate(grouped_articles, 1):\n",
        "            print(f\"[{idx}/{len(grouped_articles)}] Extracting date for: {article['Title'][:50]}...\")\n",
        "            pub_date = extract_publish_date(article['Link'])\n",
        "            article['Date'] = pub_date if pub_date else 'N/A'\n",
        "            time.sleep(0.5)  # Delay between requests\n",
        "        \n",
        "        print(\"\\n✓ Date extraction completed\")\n",
        "        \n",
        "        # Filter by date range\n",
        "        if FILTER_BY_DATE:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Filtering articles by date range: {START_DATE.strftime('%Y-%m-%d')} to {END_DATE.strftime('%Y-%m-%d')}\")\n",
        "            print(f\"{'='*60}\\n\")\n",
        "            \n",
        "            articles_before_date_filter = len(grouped_articles)\n",
        "            grouped_articles = [art for art in grouped_articles if is_date_in_range(art['Date'])]\n",
        "            \n",
        "            print(f\"Articles before date filter: {articles_before_date_filter}\")\n",
        "            print(f\"Articles after date filter: {len(grouped_articles)}\")\n",
        "        else:\n",
        "            print(f\"\\n⚠ Date filtering is disabled (FILTER_BY_DATE=False)\")\n",
        "    \n",
        "    # Save to single CSV file\n",
        "    output_filename = \"titles_vn30_cafef.csv\"\n",
        "    if grouped_articles:\n",
        "        df_output = pd.DataFrame(grouped_articles)\n",
        "        # Reorder columns to have Date after Stock_Code\n",
        "        cols = ['Stock_Code', 'Date', 'Title', 'Link', 'Summary']\n",
        "        df_output = df_output[cols]\n",
        "        df_output.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "        print(f\"✓ Saved to '{output_filename}'\")\n",
        "        \n",
        "        # Recalculate statistics from date-filtered articles\n",
        "        code_stats_filtered = {}\n",
        "        for article in grouped_articles:\n",
        "            code = article['Stock_Code']\n",
        "            code_stats_filtered[code] = code_stats_filtered.get(code, 0) + 1\n",
        "        \n",
        "        # Print statistics by stock code\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Statistics by stock code (after date filtering):\")\n",
        "        print(f\"{'='*60}\")\n",
        "        for code in sorted(code_stats_filtered.keys()):\n",
        "            count = code_stats_filtered[code]\n",
        "            if count > 0:\n",
        "                print(f\"{code}: {count} articles\")\n",
        "        \n",
        "        # Display sample\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Sample articles (first 5):\")\n",
        "        print(f\"{'='*60}\")\n",
        "        for idx, row in df_output.head(5).iterrows():\n",
        "            print(f\"\\n[{row['Stock_Code']}] {row['Date']} - {row['Title']}\")\n",
        "            print(f\"Link: {row['Link']}\")\n",
        "    else:\n",
        "        print(\"No articles matched VN30 keywords or date range!\")\n",
        "else:\n",
        "    print(\"No articles collected to filter!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vn30_crawler",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
