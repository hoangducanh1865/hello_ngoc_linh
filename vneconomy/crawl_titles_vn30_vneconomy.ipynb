{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/MTS800/tmp/hust/study_at_hust/semesters/2025_1/DS/hello_ngoc_linh/vneconomy\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.14.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: openpyxl in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.8.1)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: selenium in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.38.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.12.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et_xmlfile in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: openpyxl in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.8.1)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: selenium in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.38.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.12.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et_xmlfile in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 requests pandas numpy openpyxl openai python-dotenv selenium webdriver-manager httpx pydantic tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries for web scraping and API calls\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for vneconomy website\n",
    "BASE_URL = \"https://vneconomy.vn/\"\n",
    "\n",
    "# Mapping of VN30 stock codes to their related keywords\n",
    "KEYWORDS_MAP = {\n",
    "    \"ACB\": [\"ACB\", \"Ngân hàng ACB\", \"Ngân hàng TMCP Á Châu\"],\n",
    "    \"BCM\": [\"BCM\", \"Becamex\", \"KCN Bình Dương\", \"khu công nghiệp Bình Dương\", \"VSIP\", \"Becamex IDC\"],\n",
    "    \"BID\": [\"BIDV\", \"Ngân hàng Đầu tư và Phát triển Việt Nam\"],\n",
    "    \"CTG\": [\"CTG\", \"VietinBank\", \"Ngân hàng Công Thương Việt Nam\"],\n",
    "    \"DGC\": [\"DGC\", \"Hóa chất Đức Giang\"],\n",
    "    \"FPT\": [\"FPT\"],\n",
    "    \"GAS\": [\"PV GAS\", \"PV Gas\", \"Tổng Công ty Khí Việt Nam\"],\n",
    "    \"GVR\": [\"GVR\", \"Tập đoàn Cao su\", \"Tập đoàn Công nghiệp Cao su Việt Nam\"],\n",
    "    \"HDB\": [\"HDB\", \"HDBank\", \"Ngân hàng TMCP Phát triển Thành phố Hồ Chí Minh\"],\n",
    "    \"HPG\": [\"HPG\", \"Hòa Phát\"],\n",
    "    \"LPB\": [\"LPB\", \"LPBank\", \"LienVietPostBank\", \"Ngân hàng Bưu điện Liên Việt\"],\n",
    "    \"MBB\": [\"MBB\", \"MBBank\", \"Ngân hàng Quân đội\", \"MB\", \"Ngân hàng TMCP Quân đội\"],\n",
    "    \"MSN\": [\"MSN\", \"Masan\", \"WinCommerce\"],\n",
    "    \"MWG\": [\"MWG\", \"Thế Giới Di Động\", \"Mobile World\", \"Bách Hóa Xanh\", \"BHX\", \"Điện Máy Xanh\", \"ĐMX\", \"TGDĐ\"],\n",
    "    \"PLX\": [\"PLX\", \"Petrolimex\", \"Tập đoàn Xăng dầu Việt Nam\"],\n",
    "    \"SAB\": [\"SAB\", \"Sabeco\", \"Tổng Công ty CP Bia - Rượu - Nước giải khát Sài Gòn\"],\n",
    "    \"SHB\": [\"SHB\", \"Ngân hàng Thương mại Cổ phần Sài Gòn – Hà Nội\", \"Ngân hàng TMCP Sài Gòn Hà Nội\"],\n",
    "    \"SSB\": [\"SSB\", \"Ngân hàng Thương mại Cổ phần Đông Nam Á\", \"Ngân hàng TMCP Đông Nam Á\", \"SeABank\"],\n",
    "    \"SSI\": [\"SSI\", \"Chứng khoán SSI\"],\n",
    "    \"STB\": [\"STB\", \"Sài Gòn Thương Tín\", \"Sacombank\"],\n",
    "    \"TCB\": [\"TCB\", \"Techcombank\", \"Ngân hàng TMCP Kỹ Thương Việt Nam\"],\n",
    "    \"TPB\": [\"TPB\", \"TPBank\", \"Ngân hàng Tiên Phong\", \"Ngân hàng TMCP Tiên Phong\"],\n",
    "    \"VCB\": [\"VCB\", \"Vietcombank\", \"Ngân hàng TMCP Ngoại Thương Việt Nam\", \"Ngân hàng Ngoại thương\"],\n",
    "    \"VHM\": [\"VHM\", \"Vinhomes\"],\n",
    "    \"VIB\": [\"VIB\", \"Ngân hàng TMCP Quốc Tế Việt Nam\", \"Ngân hàng Quốc Tế\"],\n",
    "    \"VIC\": [\"VIC\", \"Vingroup\", \"Công ty Cổ phần Tập đoàn Vingroup\"],\n",
    "    \"VJC\": [\"VJC\", \"Vietjet Air\", \"Công ty Cổ phần Hàng không Vietjet\", \"máy bay Vietjet\"],\n",
    "    \"VNM\": [\"VNM\", \"Vinamilk\", \"Công ty Cổ phần Sữa Việt Nam\"],\n",
    "    \"VPB\": [\"VPB\", \"VPBank\", \"Ngân hàng TMCP Việt Nam Thịnh Vượng\"],\n",
    "    \"VRE\": [\"VRE\", \"Vincom Retail\", \"Công ty Cổ phần Vincom Retail\"]\n",
    "}\n",
    "\n",
    "# Limit category option\n",
    "LIMIT_CATEGORY = True\n",
    "NUM_CATEGORY = 5\n",
    "MAX_PAGES_PER_CATEGORY = 5\n",
    "\n",
    "# Date filtering option\n",
    "FILTER_BY_DATE = False  # Set to False to disable date filtering\n",
    "\n",
    "# Date range filter (inclusive)\n",
    "START_DATE = datetime(2020, 1, 1)\n",
    "END_DATE = datetime(2024, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Categories:\n",
      "- Chứng khoán: https://vneconomy.vn/chung-khoan.htm\n",
      "- Tiêu & Dùng: https://vneconomy.vn/tieu-dung.htm\n",
      "- VnE TV: https://vneconomy.vn/video.htm\n",
      "- eMagazine: https://vneconomy.vn/emagazine.htm\n",
      "- Infographics: https://vneconomy.vn/infographics.htm\n",
      "- Kinh tế xanh: https://vneconomy.vn/kinh-te-xanh.htm\n",
      "- Chuyển động xanh: https://vneconomy.vn/chuyen-dong-xanh.htm\n",
      "- Pháp lý: https://vneconomy.vn/phap-ly-kinh-te-xanh.htm\n",
      "- Thương hiệu xanh: https://vneconomy.vn/thuong-hieu-xanh.htm\n",
      "- Diễn đàn: https://vneconomy.vn/dien-dan-kinh-te-xanh.htm\n",
      "- Tiêu điểm: https://vneconomy.vn/tieu-diem.htm\n",
      "- Tài chính: https://vneconomy.vn/tai-chinh.htm\n",
      "- Ngân hàng: https://vneconomy.vn/tai-chinh-ngan-hang.htm\n",
      "- Thị trường vốn: https://vneconomy.vn/thi-truong-von-tai-chinh.htm\n",
      "- Thuế: https://vneconomy.vn/thue-tai-chhinh.htm\n",
      "- Bảo hiểm: https://vneconomy.vn/bao-hiem-tai-chinh.htm\n",
      "- Doanh nghiệp niêm yết: https://vneconomy.vn/doanh-nghiep-niem-yet.htm\n",
      "- Thị trường: https://vneconomy.vn/thi-truong-chung-khoan.htm\n",
      "- Quốc tế: https://vneconomy.vn/chung-khoan-quoc-te.htm\n",
      "- Khung pháp lý: https://vneconomy.vn/khung-phap-ly-chung-khoan.htm\n",
      "- Kinh tế số: https://vneconomy.vn/kinh-te-so.htm\n",
      "- Sản phẩm - Thị trường: https://vneconomy.vn/san-pham-thi-truong-kinh-te-so.htm\n",
      "- Tài sản số: https://vneconomy.vn/tai-san-so.htm\n",
      "- Dịch vụ số: https://vneconomy.vn/dich-vu-so.htm\n",
      "- Start-up: https://vneconomy.vn/start-up.htm\n",
      "- Quản trị số: https://vneconomy.vn/quan-tri-so.htm\n",
      "- Hạ tầng: https://vneconomy.vn/dau-tu-ha-tang.htm\n",
      "- Bất động sản: https://vneconomy.vn/dia-oc.htm\n",
      "- Chính sách: https://vneconomy.vn/chinh-sach-bat-dong-san.htm\n",
      "- Thị trường: https://vneconomy.vn/thi-truong-bat-dong-san.htm\n",
      "- Dự án: https://vneconomy.vn/du-an-bat-dong-san.htm\n",
      "- Cafe BĐS: https://vneconomy.vn/cafe-bds.htm\n",
      "- Tư vấn: https://vneconomy.vn/tu-van-bat-dong-san.htm\n",
      "- Thị trường: https://vneconomy.vn/thi-truong.htm\n",
      "- Xuất nhập khẩu: https://vneconomy.vn/thi-truong-xuat-nhap-khau.htm\n",
      "- Khung pháp lý: https://vneconomy.vn/khung-phap-ly-thi-truong.htm\n",
      "- Công nghiệp: https://vneconomy.vn/thi-truong-cong-nghiep.htm\n",
      "- Thị trường: https://vneconomy.vn/kin-te-thi-truong.htm\n",
      "- Nông sản: https://vneconomy.vn/thi-truong-nong-san.htm\n",
      "- Thế giới: https://vneconomy.vn/kinh-te-the-gioi.htm\n",
      "- Kinh tế: https://vneconomy.vn/the-gioi-kinh-te.htm\n",
      "- Kinh doanh: https://vneconomy.vn/kinh-doanh-the-gioi.htm\n",
      "- Doanh nghiệp: https://vneconomy.vn/nhip-cau-doanh-nghiep.htm\n",
      "- Chuyển động: https://vneconomy.vn/chuyen-dong-doanh-nghiep.htm\n",
      "- Đối thoại: https://vneconomy.vn/doi-thoai-doanh-nghiep.htm\n",
      "- Kết nối: https://vneconomy.vn/ket-noi-doanh-nghiep.htm\n",
      "- Doanh nhân: https://vneconomy.vn/doanh-nhan.htm\n",
      "- Doanh nghiệp: https://vneconomy.vn/cong-ty-doanh-nghiep.htm\n",
      "- Ấn phẩm: https://vneconomy.vn/an-pham.htm\n",
      "- The Guide: https://vneconomy.vn/the-guide.htm\n",
      "- Tạp chí kinh tế Việt Nam: https://vneconomy.vn/tap-chi-kinh-te-viet-nam.htm\n",
      "- Tư vấn Tiêu & Dùng: https://vneconomy.vn/an-pham-tu-van-va-tieu-dung.htm\n",
      "- Multimedia: https://vneconomy.vn/multimedia.htm\n",
      "- Video: https://vneconomy.vn/video.htm\n",
      "- Đầu tư: https://vneconomy.vn/dau-tu.htm\n",
      "- Nhà đầu tư: https://vneconomy.vn/nha-dau-tu.htm\n",
      "- Hạ tầng: https://vneconomy.vn/ha-tang-dau-tu.htm\n",
      "- Địa phương: https://vneconomy.vn/dau-tu-dia-phuong.htm\n",
      "- Du lịch: https://vneconomy.vn/tieu-dung-du-lich.htm\n",
      "- Sức khỏe: https://vneconomy.vn/suc-khoe.htm\n",
      "- Thị trường: https://vneconomy.vn/thi-truong-tieu-dung.htm\n",
      "- Đẹp +: https://vneconomy.vn/dep.htm\n",
      "- Giải trí: https://vneconomy.vn/tieu-dung-giai-tri.htm\n",
      "- Nhà: https://vneconomy.vn/nha-tieu-dung.htm\n",
      "- Ẩm thực: https://vneconomy.vn/am-thuc-tieu-dung.htm\n",
      "- Công nghệ & Startup: https://vneconomy.vn/cong-nghe-startup.htm\n",
      "- Dân sinh: https://vneconomy.vn/dan-sinh.htm\n",
      "- Khung pháp lý: https://vneconomy.vn/khung-phap-ly-dan-sinh.htm\n",
      "- Bảo hiểm: https://vneconomy.vn/bao-hiem-dan-sinh.htm\n",
      "- Nhân lực: https://vneconomy.vn/nhan-luc.htm\n",
      "- An sinh: https://vneconomy.vn/an-sinh.htm\n",
      "- Y tế: https://vneconomy.vn/y-te-dan-sinh.htm\n",
      "- VnEconomy Interactive: https://vneconomy.vn/vneconomy-interactive-e320.htm\n",
      "- Sự kiện: https://vneconomy.vn/event.htm\n",
      "- Thị trường: https://vneconomy.vn/thi-truong.htm\n"
     ]
    }
   ],
   "source": [
    "def get_categories():\n",
    "    # Fetch the homepage HTML\n",
    "    html = requests.get(BASE_URL, timeout=10).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    categories = []\n",
    "\n",
    "    # Find all anchor tags with title attribute\n",
    "    for a in soup.find_all(\"a\", title=True):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        \n",
    "        # Only keep links ending with .htm\n",
    "        if href.endswith(\".htm\"):\n",
    "            title = a.get(\"title\").strip()\n",
    "\n",
    "            # Normalize the URL to absolute path\n",
    "            if href.startswith(\"/\"):\n",
    "                full_url = BASE_URL.rstrip(\"/\") + href\n",
    "            else:\n",
    "                full_url = href\n",
    "\n",
    "            categories.append({\n",
    "                \"title\": title,\n",
    "                \"url\": full_url\n",
    "            })\n",
    "\n",
    "    return categories\n",
    "\n",
    "\n",
    "def filter_category_pages(categories):\n",
    "    # Initialize OpenAI client with API key from environment variable\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # Format categories as text for GPT\n",
    "    urls_text = \"\\n\".join([f\"- {c['title']}: {c['url']}\" for c in categories])\n",
    "\n",
    "    # Call GPT to filter out article pages and keep only category pages\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant that filters a list of URLs. \"\n",
    "                           \"Keep only main category pages and remove specific news articles. \"\n",
    "                           \"Return only a JSON array of objects with 'title' and 'url'.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is the list:\\n{urls_text}\\n\\nFilter them and return JSON array.\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Parse the JSON response\n",
    "    filtered_json = response.choices[0].message.content\n",
    "    try:\n",
    "        filtered_list = json.loads(filtered_json)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON. Here's raw output:\")\n",
    "        print(filtered_json)\n",
    "        filtered_list = []\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "# Get all categories from the homepage\n",
    "cats = get_categories()\n",
    "# Filter to keep only main category pages\n",
    "filtered_cats = filter_category_pages(cats)\n",
    "\n",
    "# Display filtered categories\n",
    "print(\"Filtered Categories:\")\n",
    "for c in filtered_cats:\n",
    "    print(f\"- {c['title']}: {c['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: Chứng khoán\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=1 ...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=5 ...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm?page=5 ...\n",
      "Processing category: Tiêu & Dùng\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=1 ...\n",
      "Processing category: Tiêu & Dùng\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=1 ...\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=5 ...\n",
      "Crawling https://vneconomy.vn/tieu-dung.htm?page=5 ...\n",
      "Processing category: VnE TV\n",
      "Crawling https://vneconomy.vn/video.htm?page=1 ...\n",
      "Processing category: VnE TV\n",
      "Crawling https://vneconomy.vn/video.htm?page=1 ...\n",
      "Crawling https://vneconomy.vn/video.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/video.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/video.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/video.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/video.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/video.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/video.htm?page=5 ...\n",
      "Crawling https://vneconomy.vn/video.htm?page=5 ...\n",
      "Processing category: eMagazine\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=1 ...\n",
      "Processing category: eMagazine\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=1 ...\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=5 ...\n",
      "Crawling https://vneconomy.vn/emagazine.htm?page=5 ...\n",
      "Processing category: Infographics\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=1 ...\n",
      "Processing category: Infographics\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=1 ...\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=2 ...\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=3 ...\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=4 ...\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=5 ...\n",
      "Crawling https://vneconomy.vn/infographics.htm?page=5 ...\n",
      "Done. Total categories: 5\n",
      "Done. Total categories: 5\n"
     ]
    }
   ],
   "source": [
    "def fetch_article_summary(article_url):\n",
    "    # Fetch the meta description from article page as summary\n",
    "    try:\n",
    "        html = requests.get(article_url, timeout=10).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        if meta_desc and meta_desc.get(\"content\"):\n",
    "            return meta_desc[\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch summary for {article_url}: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def crawl_articles(category_url, max_pages=MAX_PAGES_PER_CATEGORY, delay=1):\n",
    "    # Crawl all articles from a category with pagination\n",
    "    articles = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        # Build pagination URL\n",
    "        if \"?\" in category_url:\n",
    "            url = f\"{category_url}&page={page}\"\n",
    "        else:\n",
    "            url = f\"{category_url}?page={page}\"\n",
    "\n",
    "        print(f\"Crawling {url} ...\")\n",
    "        try:\n",
    "            html = requests.get(url, timeout=10).text\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        # Find all article links with responsive-image-link class\n",
    "        article_tags = soup.find_all(\"a\", class_=\"responsive-image-link\", title=True, href=True)\n",
    "\n",
    "        # Stop if no more articles found\n",
    "        if not article_tags:\n",
    "            print(f\"  ⚠ No articles found on page {page}. Reached end of available pages.\")\n",
    "            break\n",
    "\n",
    "        for a in article_tags:\n",
    "            title = a.get(\"title\").strip()\n",
    "            href = a.get(\"href\").strip()\n",
    "            full_url = href if href.startswith(\"http\") else f\"https://vneconomy.vn{href}\"\n",
    "\n",
    "            # Fetch article summary\n",
    "            summary = fetch_article_summary(full_url)\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"url\": full_url,\n",
    "                \"summary\": summary\n",
    "            })\n",
    "\n",
    "        # Delay between requests to avoid overwhelming the server\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Crawl articles from each category\n",
    "all_articles = {}\n",
    "cnt = 0\n",
    "for cat in filtered_cats:\n",
    "    print(f\"Processing category: {cat['title']}\")\n",
    "    cat_articles = crawl_articles(cat['url'])\n",
    "    all_articles[cat['title']] = cat_articles\n",
    "    cnt += 1\n",
    "    if LIMIT_CATEGORY:\n",
    "        # Limit to specified number of categories\n",
    "        if cnt >= NUM_CATEGORY:\n",
    "            break\n",
    "\n",
    "print(\"Done. Total categories:\", len(all_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_publish_date(article_url):\n",
    "    \"\"\"\n",
    "    Extract publication date from article page's meta tag.\n",
    "    Returns date in format YYYY-MM-DD or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use proper browser headers to avoid 403 errors\n",
    "        date_headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Referer': 'https://vneconomy.vn/',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(article_url, headers=date_headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find meta tag with article:published_time\n",
    "        meta_tag = soup.find('meta', property='article:published_time')\n",
    "        if meta_tag and meta_tag.get('content'):\n",
    "            # Extract date part (YYYY-MM-DD) from datetime string\n",
    "            datetime_str = meta_tag['content']\n",
    "            date_part = datetime_str.split('T')[0]\n",
    "            return date_part\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error extracting date: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_date_in_range(date_str, start_date=START_DATE, end_date=END_DATE):\n",
    "    \"\"\"\n",
    "    Check if date string is within the specified range.\n",
    "    \"\"\"\n",
    "    if not date_str or date_str == 'N/A':\n",
    "        return False\n",
    "    try:\n",
    "        article_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        return start_date <= article_date <= end_date\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_and_group_by_codes(articles, keywords_map=KEYWORDS_MAP):\n",
    "    \"\"\"\n",
    "    Filter articles by VN30 keywords and group by stock codes.\n",
    "    Uses word boundaries to match full keywords only.\n",
    "    Returns a list of dictionaries with stock code and associated articles.\n",
    "    \"\"\"\n",
    "    # Dictionary to store articles for each code (using link as key to avoid duplicates)\n",
    "    code_articles = {code: {} for code in keywords_map.keys()}\n",
    "    \n",
    "    for art in articles:\n",
    "        # Combine title and summary for keyword matching\n",
    "        text = (art[\"title\"] + \" \" + art.get(\"summary\", \"\")).lower()\n",
    "        matched_codes = []\n",
    "        \n",
    "        # Check each stock code's keywords\n",
    "        for code, kws in keywords_map.items():\n",
    "            for kw in kws:\n",
    "                # Use word boundary regex to match full words only\n",
    "                pattern = r'\\b' + re.escape(kw.lower()) + r'\\b'\n",
    "                if re.search(pattern, text):\n",
    "                    matched_codes.append(code)\n",
    "                    break\n",
    "        \n",
    "        # Add article to each matched code's dictionary (using link as key to prevent duplicates)\n",
    "        article_link = art[\"url\"]\n",
    "        for code in matched_codes:\n",
    "            if article_link and article_link not in code_articles[code]:\n",
    "                code_articles[code][article_link] = {\n",
    "                    'Stock_Code': code,\n",
    "                    'Title': art[\"title\"],\n",
    "                    'Link': article_link,\n",
    "                    'Summary': art.get(\"summary\", \"N/A\")\n",
    "                }\n",
    "    \n",
    "    # Flatten into single list, sorted by stock code\n",
    "    result = []\n",
    "    for code in sorted(code_articles.keys()):\n",
    "        result.extend(code_articles[code].values())\n",
    "    \n",
    "    return result, {code: list(articles.values()) for code, articles in code_articles.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Filtering and grouping articles by VN30 stock codes...\n",
      "============================================================\n",
      "\n",
      "Total articles before filtering: 835\n",
      "Articles matching VN30 keywords: 7\n",
      "\n",
      "============================================================\n",
      "Extracting publication dates from article pages...\n",
      "============================================================\n",
      "\n",
      "[1/7] Extracting date for: NIM toàn ngành ngân hàng về mức thấp nhất kể từ cu...\n",
      "Articles matching VN30 keywords: 7\n",
      "\n",
      "============================================================\n",
      "Extracting publication dates from article pages...\n",
      "============================================================\n",
      "\n",
      "[1/7] Extracting date for: NIM toàn ngành ngân hàng về mức thấp nhất kể từ cu...\n",
      "[2/7] Extracting date for: Hơn một thập kỷ đồng hành cùng doanh nghiệp xuất n...\n",
      "[2/7] Extracting date for: Hơn một thập kỷ đồng hành cùng doanh nghiệp xuất n...\n",
      "[3/7] Extracting date for: Masan Consumer duy trì lợi nhuận bền vững giữa biế...\n",
      "[3/7] Extracting date for: Masan Consumer duy trì lợi nhuận bền vững giữa biế...\n",
      "[4/7] Extracting date for: SHB được dự báo vào rổ chỉ số toàn cầu FTSE Global...\n",
      "[4/7] Extracting date for: SHB được dự báo vào rổ chỉ số toàn cầu FTSE Global...\n",
      "[5/7] Extracting date for: Cổ phiếu blue-chips dẫn sóng, VN-Index đột phá mốc...\n",
      "[5/7] Extracting date for: Cổ phiếu blue-chips dẫn sóng, VN-Index đột phá mốc...\n",
      "[6/7] Extracting date for: Cổ phiếu blue-chips dẫn sóng, VN-Index đột phá mốc...\n",
      "[6/7] Extracting date for: Cổ phiếu blue-chips dẫn sóng, VN-Index đột phá mốc...\n",
      "[7/7] Extracting date for: Cổ phiếu blue-chips dẫn sóng, VN-Index đột phá mốc...\n",
      "[7/7] Extracting date for: Cổ phiếu blue-chips dẫn sóng, VN-Index đột phá mốc...\n",
      "\n",
      "✓ Date extraction completed\n",
      "\n",
      "⚠ Date filtering is disabled (FILTER_BY_DATE=False)\n",
      "✓ Saved to 'titles_vn30_vneconomy.csv'\n",
      "\n",
      "============================================================\n",
      "Statistics by stock code (after date filtering):\n",
      "============================================================\n",
      "HDB: 1 articles\n",
      "MBB: 1 articles\n",
      "MSN: 1 articles\n",
      "SHB: 1 articles\n",
      "VHM: 1 articles\n",
      "VIC: 1 articles\n",
      "VPB: 1 articles\n",
      "\n",
      "============================================================\n",
      "Sample articles (first 5):\n",
      "============================================================\n",
      "\n",
      "[HDB] 2025-11-17 - NIM toàn ngành ngân hàng về mức thấp nhất kể từ cuối 2018, rủi ro định giá cổ phiếu\n",
      "Link: https://vneconomy.vn/nim-toan-nganh-ngan-hang-ve-muc-thap-nhat-ke-tu-cuoi-2018-rui-ro-dinh-gia-co-phieu.htm\n",
      "\n",
      "[MBB] 2025-11-17 - Hơn một thập kỷ đồng hành cùng doanh nghiệp xuất nhập khẩu kiến tạo tăng trưởng bền vững\n",
      "Link: https://vneconomy.vn/hon-mot-thap-ky-dong-hanh-cung-doanh-nghiep-xuat-nhap-khau-kien-tao-tang-truong-ben-vung.htm\n",
      "\n",
      "[MSN] 2025-11-20 - Masan Consumer duy trì lợi nhuận bền vững giữa biến động toàn cầu\n",
      "Link: https://vneconomy.vn/masan-consumer-duy-tri-loi-nhuan-ben-vung-giua-bien-dong-toan-cau.htm\n",
      "\n",
      "[SHB] 2025-11-14 - SHB được dự báo vào rổ chỉ số toàn cầu FTSE Global All Cap\n",
      "Link: https://vneconomy.vn/shb-duoc-du-bao-vao-ro-chi-so-toan-cau-ftse-global-all-cap.htm\n",
      "\n",
      "[VHM] 2025-11-17 - Cổ phiếu blue-chips dẫn sóng, VN-Index đột phá mốc 1650 điểm\n",
      "Link: https://vneconomy.vn/co-phieu-blue-chips-dan-song-vn-index-dot-pha-moc-1650-diem.htm\n",
      "\n",
      "✓ Date extraction completed\n",
      "\n",
      "⚠ Date filtering is disabled (FILTER_BY_DATE=False)\n",
      "✓ Saved to 'titles_vn30_vneconomy.csv'\n",
      "\n",
      "============================================================\n",
      "Statistics by stock code (after date filtering):\n",
      "============================================================\n",
      "HDB: 1 articles\n",
      "MBB: 1 articles\n",
      "MSN: 1 articles\n",
      "SHB: 1 articles\n",
      "VHM: 1 articles\n",
      "VIC: 1 articles\n",
      "VPB: 1 articles\n",
      "\n",
      "============================================================\n",
      "Sample articles (first 5):\n",
      "============================================================\n",
      "\n",
      "[HDB] 2025-11-17 - NIM toàn ngành ngân hàng về mức thấp nhất kể từ cuối 2018, rủi ro định giá cổ phiếu\n",
      "Link: https://vneconomy.vn/nim-toan-nganh-ngan-hang-ve-muc-thap-nhat-ke-tu-cuoi-2018-rui-ro-dinh-gia-co-phieu.htm\n",
      "\n",
      "[MBB] 2025-11-17 - Hơn một thập kỷ đồng hành cùng doanh nghiệp xuất nhập khẩu kiến tạo tăng trưởng bền vững\n",
      "Link: https://vneconomy.vn/hon-mot-thap-ky-dong-hanh-cung-doanh-nghiep-xuat-nhap-khau-kien-tao-tang-truong-ben-vung.htm\n",
      "\n",
      "[MSN] 2025-11-20 - Masan Consumer duy trì lợi nhuận bền vững giữa biến động toàn cầu\n",
      "Link: https://vneconomy.vn/masan-consumer-duy-tri-loi-nhuan-ben-vung-giua-bien-dong-toan-cau.htm\n",
      "\n",
      "[SHB] 2025-11-14 - SHB được dự báo vào rổ chỉ số toàn cầu FTSE Global All Cap\n",
      "Link: https://vneconomy.vn/shb-duoc-du-bao-vao-ro-chi-so-toan-cau-ftse-global-all-cap.htm\n",
      "\n",
      "[VHM] 2025-11-17 - Cổ phiếu blue-chips dẫn sóng, VN-Index đột phá mốc 1650 điểm\n",
      "Link: https://vneconomy.vn/co-phieu-blue-chips-dan-song-vn-index-dot-pha-moc-1650-diem.htm\n"
     ]
    }
   ],
   "source": [
    "# Flatten all articles from all categories into a single list\n",
    "all_articles_list = []\n",
    "for cat_articles in all_articles.values():\n",
    "    all_articles_list.extend(cat_articles)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Filtering and grouping articles by VN30 stock codes...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"Total articles before filtering: {len(all_articles_list)}\")\n",
    "\n",
    "# Filter and group by codes\n",
    "grouped_articles, code_dict = filter_and_group_by_codes(all_articles_list, keywords_map=KEYWORDS_MAP)\n",
    "\n",
    "print(f\"Articles matching VN30 keywords: {len(grouped_articles)}\")\n",
    "\n",
    "# Extract publication dates for filtered articles\n",
    "if grouped_articles:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Extracting publication dates from article pages...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx, article in enumerate(grouped_articles, 1):\n",
    "        print(f\"[{idx}/{len(grouped_articles)}] Extracting date for: {article['Title'][:50]}...\")\n",
    "        pub_date = extract_publish_date(article['Link'])\n",
    "        article['Date'] = pub_date if pub_date else 'N/A'\n",
    "        time.sleep(0.5)  # Delay between requests\n",
    "    \n",
    "    print(\"\\n✓ Date extraction completed\")\n",
    "    \n",
    "    # Filter by date range\n",
    "    if FILTER_BY_DATE:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Filtering articles by date range: {START_DATE.strftime('%Y-%m-%d')} to {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        articles_before_date_filter = len(grouped_articles)\n",
    "        grouped_articles = [art for art in grouped_articles if is_date_in_range(art['Date'])]\n",
    "        \n",
    "        print(f\"Articles before date filter: {articles_before_date_filter}\")\n",
    "        print(f\"Articles after date filter: {len(grouped_articles)}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Date filtering is disabled (FILTER_BY_DATE=False)\")\n",
    "\n",
    "# Save to single CSV file\n",
    "output_filename = \"titles_vn30_vneconomy.csv\"\n",
    "if grouped_articles:\n",
    "    df_output = pd.DataFrame(grouped_articles)\n",
    "    # Reorder columns to have Date after Stock_Code\n",
    "    cols = ['Stock_Code', 'Date', 'Title', 'Link', 'Summary']\n",
    "    df_output = df_output[cols]\n",
    "    df_output.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✓ Saved to '{output_filename}'\")\n",
    "    \n",
    "    # Recalculate statistics from date-filtered articles\n",
    "    code_stats_filtered = {}\n",
    "    for article in grouped_articles:\n",
    "        code = article['Stock_Code']\n",
    "        code_stats_filtered[code] = code_stats_filtered.get(code, 0) + 1\n",
    "    \n",
    "    # Print statistics by stock code\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Statistics by stock code (after date filtering):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for code in sorted(code_stats_filtered.keys()):\n",
    "        count = code_stats_filtered[code]\n",
    "        if count > 0:\n",
    "            print(f\"{code}: {count} articles\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Sample articles (first 5):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for idx, row in df_output.head(5).iterrows():\n",
    "        print(f\"\\n[{row['Stock_Code']}] {row['Date']} - {row['Title']}\")\n",
    "        print(f\"Link: {row['Link']}\")\n",
    "else:\n",
    "    print(\"No articles matched VN30 keywords or date range!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vn30_crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
