{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34f274ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/MTS800/tmp/hust/study_at_hust/semesters/2025_1/DS/hello_ngoc_linh/vietstock\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "192430da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.14.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: openpyxl in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.8.1)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: selenium in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.38.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (2.12.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et_xmlfile in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et_xmlfile in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/vn30_crawler/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 requests pandas numpy openpyxl openai python-dotenv selenium webdriver-manager httpx pydantic tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9c70757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3785ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_CATEGORY = True\n",
    "NUM_CATEGORIES = 5\n",
    "\n",
    "# Maximum pages to crawl per category\n",
    "MAX_PAGE_PER_CATEGORY = 5\n",
    "\n",
    "# Date filtering option\n",
    "FILTER_BY_DATE = False  # Set to False to disable date filtering\n",
    "\n",
    "# Date range filter (inclusive)\n",
    "START_DATE = datetime(2020, 1, 1)\n",
    "END_DATE = datetime(2024, 12, 31)\n",
    "\n",
    "# Mapping of VN30 stock codes to their related keywords\n",
    "KEYWORDS_MAP = {\n",
    "    \"ACB\": [\"ACB\", \"Ngân hàng ACB\", \"Ngân hàng TMCP Á Châu\"],\n",
    "    \"BCM\": [\"BCM\", \"Becamex\", \"KCN Bình Dương\", \"khu công nghiệp Bình Dương\", \"VSIP\", \"Becamex IDC\"],\n",
    "    \"BID\": [\"BIDV\", \"Ngân hàng Đầu tư và Phát triển Việt Nam\"],\n",
    "    \"CTG\": [\"CTG\", \"VietinBank\", \"Ngân hàng Công Thương Việt Nam\"],\n",
    "    \"DGC\": [\"DGC\", \"Hóa chất Đức Giang\"],\n",
    "    \"FPT\": [\"FPT\"],\n",
    "    \"GAS\": [\"PV GAS\", \"PV Gas\", \"Tổng Công ty Khí Việt Nam\"],\n",
    "    \"GVR\": [\"GVR\", \"Tập đoàn Cao su\", \"Tập đoàn Công nghiệp Cao su Việt Nam\"],\n",
    "    \"HDB\": [\"HDB\", \"HDBank\", \"Ngân hàng TMCP Phát triển Thành phố Hồ Chí Minh\"],\n",
    "    \"HPG\": [\"HPG\", \"Hòa Phát\"],\n",
    "    \"LPB\": [\"LPB\", \"LPBank\", \"LienVietPostBank\", \"Ngân hàng Bưu điện Liên Việt\"],\n",
    "    \"MBB\": [\"MBB\", \"MBBank\", \"Ngân hàng Quân đội\", \"MB\", \"Ngân hàng TMCP Quân đội\"],\n",
    "    \"MSN\": [\"MSN\", \"Masan\", \"WinCommerce\"],\n",
    "    \"MWG\": [\"MWG\", \"Thế Giới Di Động\", \"Mobile World\", \"Bách Hóa Xanh\", \"BHX\", \"Điện Máy Xanh\", \"ĐMX\", \"TGDĐ\"],\n",
    "    \"PLX\": [\"PLX\", \"Petrolimex\", \"Tập đoàn Xăng dầu Việt Nam\"],\n",
    "    \"SAB\": [\"SAB\", \"Sabeco\", \"Tổng Công ty CP Bia - Rượu - Nước giải khát Sài Gòn\"],\n",
    "    \"SHB\": [\"SHB\", \"Ngân hàng Thương mại Cổ phần Sài Gòn – Hà Nội\", \"Ngân hàng TMCP Sài Gòn Hà Nội\"],\n",
    "    \"SSB\": [\"SSB\", \"Ngân hàng Thương mại Cổ phần Đông Nam Á\", \"Ngân hàng TMCP Đông Nam Á\", \"SeABank\"],\n",
    "    \"SSI\": [\"SSI\", \"Chứng khoán SSI\"],\n",
    "    \"STB\": [\"STB\", \"Sài Gòn Thương Tín\", \"Sacombank\"],\n",
    "    \"TCB\": [\"TCB\", \"Techcombank\", \"Ngân hàng TMCP Kỹ Thương Việt Nam\"],\n",
    "    \"TPB\": [\"TPB\", \"TPBank\", \"Ngân hàng Tiên Phong\", \"Ngân hàng TMCP Tiên Phong\"],\n",
    "    \"VCB\": [\"VCB\", \"Vietcombank\", \"Ngân hàng TMCP Ngoại Thương Việt Nam\", \"Ngân hàng Ngoại thương\"],\n",
    "    \"VHM\": [\"VHM\", \"Vinhomes\"],\n",
    "    \"VIB\": [\"VIB\", \"Ngân hàng TMCP Quốc Tế Việt Nam\", \"Ngân hàng Quốc Tế\"],\n",
    "    \"VIC\": [\"VIC\", \"Vingroup\", \"Công ty Cổ phần Tập đoàn Vingroup\"],\n",
    "    \"VJC\": [\"VJC\", \"Vietjet Air\", \"Công ty Cổ phần Hàng không Vietjet\", \"máy bay Vietjet\"],\n",
    "    \"VNM\": [\"VNM\", \"Vinamilk\", \"Công ty Cổ phần Sữa Việt Nam\"],\n",
    "    \"VPB\": [\"VPB\", \"VPBank\", \"Ngân hàng TMCP Việt Nam Thịnh Vượng\"],\n",
    "    \"VRE\": [\"VRE\", \"Vincom Retail\", \"Công ty Cổ phần Vincom Retail\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c55c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_articles_by_keywords(articles, keywords_map=KEYWORDS_MAP):\n",
    "    # Filter articles containing VN30 stock keywords\n",
    "    filtered = []\n",
    "    \n",
    "    for art in articles:\n",
    "        # Convert title to lowercase for case-insensitive matching\n",
    "        text = art[\"title\"].lower()\n",
    "        matched_codes = []\n",
    "        \n",
    "        # Check each stock code\n",
    "        for code, keywords in keywords_map.items():\n",
    "            for keyword in keywords:\n",
    "                # Use word boundary regex to match full words only\n",
    "                pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n",
    "                if re.search(pattern, text):\n",
    "                    matched_codes.append(code)\n",
    "                    break\n",
    "        \n",
    "        # Keep articles with at least one matched stock code\n",
    "        if matched_codes:\n",
    "            art_copy = art.copy()\n",
    "            art_copy[\"codes\"] = list(set(matched_codes))  # Remove duplicates\n",
    "            filtered.append(art_copy)\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4f929a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting crawl process\n",
      "Max pages per category: 5\n",
      "Total categories: 14\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/14] Category: https://vietstock.vn/chung-khoan.htm\n",
      "Channel ID: 144\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/chung-khoan.htm\n",
      "Token obtained: -k6MVafOu7tzemjgDz6h...\n",
      "  Crawling page 1/5...\n",
      "Token obtained: -k6MVafOu7tzemjgDz6h...\n",
      "  Crawling page 1/5...\n",
      "  ✓ Found 10 articles on page 1\n",
      "  ✓ Found 10 articles on page 1\n",
      "  Crawling page 2/5...\n",
      "  Crawling page 2/5...\n",
      "  ✓ Found 10 articles on page 2\n",
      "  ✓ Found 10 articles on page 2\n",
      "  Crawling page 3/5...\n",
      "  Crawling page 3/5...\n",
      "  ✓ Found 10 articles on page 3\n",
      "  ✓ Found 10 articles on page 3\n",
      "  Crawling page 4/5...\n",
      "  Crawling page 4/5...\n",
      "  ✓ Found 10 articles on page 4\n",
      "  ✓ Found 10 articles on page 4\n",
      "  Crawling page 5/5...\n",
      "  Crawling page 5/5...\n",
      "  ✓ Found 10 articles on page 5\n",
      "  ✓ Found 10 articles on page 5\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 50\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 50\n",
      "\n",
      "[2/14] Category: https://vietstock.vn/doanh-nghiep.htm\n",
      "Channel ID: 733\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/doanh-nghiep.htm\n",
      "\n",
      "[2/14] Category: https://vietstock.vn/doanh-nghiep.htm\n",
      "Channel ID: 733\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/doanh-nghiep.htm\n",
      "Token obtained: rPF3Aba8LT1nbnnxQZwi...\n",
      "  Crawling page 1/5...\n",
      "  ✓ Found 10 articles on page 1\n",
      "Token obtained: rPF3Aba8LT1nbnnxQZwi...\n",
      "  Crawling page 1/5...\n",
      "  ✓ Found 10 articles on page 1\n",
      "  Crawling page 2/5...\n",
      "  Crawling page 2/5...\n",
      "  ✓ Found 10 articles on page 2\n",
      "  ✓ Found 10 articles on page 2\n",
      "  Crawling page 3/5...\n",
      "  Crawling page 3/5...\n",
      "  ✓ Found 10 articles on page 3\n",
      "  ✓ Found 10 articles on page 3\n",
      "  Crawling page 4/5...\n",
      "  Crawling page 4/5...\n",
      "  ✓ Found 10 articles on page 4\n",
      "  ✓ Found 10 articles on page 4\n",
      "  Crawling page 5/5...\n",
      "  Crawling page 5/5...\n",
      "  ✓ Found 10 articles on page 5\n",
      "  ✓ Found 10 articles on page 5\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 100\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 100\n",
      "\n",
      "[3/14] Category: https://vietstock.vn/bat-dong-san.htm\n",
      "Channel ID: 763\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/bat-dong-san.htm\n",
      "\n",
      "[3/14] Category: https://vietstock.vn/bat-dong-san.htm\n",
      "Channel ID: 763\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/bat-dong-san.htm\n",
      "Token obtained: j9tO_pgyQjwk1XKldwPt...\n",
      "  Crawling page 1/5...\n",
      "  ✓ Found 10 articles on page 1\n",
      "Token obtained: j9tO_pgyQjwk1XKldwPt...\n",
      "  Crawling page 1/5...\n",
      "  ✓ Found 10 articles on page 1\n",
      "  Crawling page 2/5...\n",
      "  Crawling page 2/5...\n",
      "  ✓ Found 10 articles on page 2\n",
      "  ✓ Found 10 articles on page 2\n",
      "  Crawling page 3/5...\n",
      "  Crawling page 3/5...\n",
      "  ✓ Found 10 articles on page 3\n",
      "  ✓ Found 10 articles on page 3\n",
      "  Crawling page 4/5...\n",
      "  Crawling page 4/5...\n",
      "  ✓ Found 10 articles on page 4\n",
      "  ✓ Found 10 articles on page 4\n",
      "  Crawling page 5/5...\n",
      "  Crawling page 5/5...\n",
      "  ✓ Found 10 articles on page 5\n",
      "  ✓ Found 10 articles on page 5\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 150\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 150\n",
      "\n",
      "[4/14] Category: https://vietstock.vn/tai-chinh.htm\n",
      "Channel ID: 734\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/tai-chinh.htm\n",
      "\n",
      "[4/14] Category: https://vietstock.vn/tai-chinh.htm\n",
      "Channel ID: 734\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/tai-chinh.htm\n",
      "Token obtained: 9ufISq8hgGFMVOdypRD9...\n",
      "  Crawling page 1/5...\n",
      "Token obtained: 9ufISq8hgGFMVOdypRD9...\n",
      "  Crawling page 1/5...\n",
      "  ✓ Found 10 articles on page 1\n",
      "  ✓ Found 10 articles on page 1\n",
      "  Crawling page 2/5...\n",
      "  Crawling page 2/5...\n",
      "  ✓ Found 10 articles on page 2\n",
      "  ✓ Found 10 articles on page 2\n",
      "  Crawling page 3/5...\n",
      "  Crawling page 3/5...\n",
      "  ✓ Found 10 articles on page 3\n",
      "  ✓ Found 10 articles on page 3\n",
      "  Crawling page 4/5...\n",
      "  Crawling page 4/5...\n",
      "  ✓ Found 10 articles on page 4\n",
      "  ✓ Found 10 articles on page 4\n",
      "  Crawling page 5/5...\n",
      "  Crawling page 5/5...\n",
      "  ✓ Found 10 articles on page 5\n",
      "  ✓ Found 10 articles on page 5\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 200\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 200\n",
      "\n",
      "[5/14] Category: https://vietstock.vn/hang-hoa.htm\n",
      "Channel ID: 2\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/hang-hoa.htm\n",
      "\n",
      "[5/14] Category: https://vietstock.vn/hang-hoa.htm\n",
      "Channel ID: 2\n",
      "------------------------------------------------------------\n",
      "Accessing main page: https://vietstock.vn/hang-hoa.htm\n",
      "Token obtained: fKkmZUHO4JX0kGzXWzGX...\n",
      "  Crawling page 1/5...\n",
      "Token obtained: fKkmZUHO4JX0kGzXWzGX...\n",
      "  Crawling page 1/5...\n",
      "  ✓ Found 10 articles on page 1\n",
      "  ✓ Found 10 articles on page 1\n",
      "  Crawling page 2/5...\n",
      "  Crawling page 2/5...\n",
      "  ✓ Found 10 articles on page 2\n",
      "  ✓ Found 10 articles on page 2\n",
      "  Crawling page 3/5...\n",
      "  Crawling page 3/5...\n",
      "  ✓ Found 10 articles on page 3\n",
      "  ✓ Found 10 articles on page 3\n",
      "  Crawling page 4/5...\n",
      "  Crawling page 4/5...\n",
      "  ✓ Found 10 articles on page 4\n",
      "  ✓ Found 10 articles on page 4\n",
      "  Crawling page 5/5...\n",
      "  Crawling page 5/5...\n",
      "  ✓ Found 10 articles on page 5\n",
      "  ✓ Found 10 articles on page 5\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 250\n",
      "Completed category: 50 total articles\n",
      "Total articles so far: 250\n",
      "\n",
      "⚠ Category limit reached (5). Stopping...\n",
      "\n",
      "============================================================\n",
      "✓ Crawling completed!\n",
      "Total articles saved: 250\n",
      "Output file: vietstock_articles.jsonl\n",
      "============================================================\n",
      "\n",
      "⚠ Category limit reached (5). Stopping...\n",
      "\n",
      "============================================================\n",
      "✓ Crawling completed!\n",
      "Total articles saved: 250\n",
      "Output file: vietstock_articles.jsonl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# HTTP headers to simulate a real browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'X-Requested-With': 'XMLHttpRequest',\n",
    "    'Origin': 'https://vietstock.vn',\n",
    "    'Referer': 'https://vietstock.vn/tai-chinh.htm',\n",
    "}\n",
    "\n",
    "# Mapping of category URLs to their channel IDs\n",
    "CATEGORY_CHANNEL_MAP = {\n",
    "    'https://vietstock.vn/chung-khoan.htm': '144',\n",
    "    'https://vietstock.vn/doanh-nghiep.htm': '733',\n",
    "    'https://vietstock.vn/bat-dong-san.htm': '763',\n",
    "    'https://vietstock.vn/tai-chinh.htm': '734',\n",
    "    'https://vietstock.vn/hang-hoa.htm': '2',\n",
    "    'https://vietstock.vn/kinh-te/vi-mo.htm': '761',\n",
    "    'https://vietstock.vn/kinh-te/kinh-te-dau-tu.htm': '768',\n",
    "    'https://vietstock.vn/the-gioi.htm': '736',\n",
    "    'https://vietstock.vn/dong-duong.htm': '1317',\n",
    "    'https://vietstock.vn/tai-chinh-ca-nhan.htm': '4259',\n",
    "    'https://vietstock.vn/nhan-dinh-phan-tich.htm': '579',\n",
    "    'https://vietstock.vn/san-giao-dich-tai-chinh.htm': '4645',\n",
    "    'https://vietstock.vn/kinh-te.htm': '5307',\n",
    "    'https://vietstock.vn/goc-nhin.htm': '4314',\n",
    "}\n",
    "\n",
    "def crawl_vietstock_category(url_category, channel_id, max_pages=3, output_file=None):\n",
    "    # Update referer header for this category\n",
    "    headers['Referer']=url_category\n",
    "    session = requests.Session()\n",
    "    articles_data = []\n",
    "    \n",
    "    # Step 1: Request the main page to get cookies\n",
    "    print(f\"Accessing main page: {url_category}\")\n",
    "    response = session.get(url_category, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access main page (status: {response.status_code})\")\n",
    "        return articles_data\n",
    "\n",
    "    soup_main = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Get authentication token if available\n",
    "    token = \"\"\n",
    "    token_tag = soup_main.find('input', {'name': '__RequestVerificationToken'})\n",
    "    if token_tag:\n",
    "        token = token_tag.get('value')\n",
    "        print(f\"Token obtained: {token[:20]}...\")\n",
    "\n",
    "    # Step 2: Loop through pages (pagination)\n",
    "    api_url = \"https://vietstock.vn/StartPage/ChannelContentPage\"\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"  Crawling page {page}/{max_pages}...\")\n",
    "        \n",
    "        # Prepare payload for API request\n",
    "        payload = {\n",
    "            'channelID': channel_id,\n",
    "            'page': page,\n",
    "            '__RequestVerificationToken': token\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            post_response = session.post(api_url, data=payload, headers=headers)\n",
    "            \n",
    "            if post_response.status_code == 200:\n",
    "                soup_api = BeautifulSoup(post_response.text, 'html.parser')\n",
    "                # Find all article titles\n",
    "                articles = soup_api.find_all('h4', class_='channel-title') \n",
    "                \n",
    "                if not articles:\n",
    "                    articles = soup_api.select('h4 > a')\n",
    "\n",
    "                # Check if page is empty (no more articles)\n",
    "                if not articles:\n",
    "                    print(f\"  ⚠ No articles found on page {page}. Reached end of available pages.\")\n",
    "                    break\n",
    "\n",
    "                page_articles_count = 0\n",
    "                # Extract article data\n",
    "                for article in articles:\n",
    "                    link_tag = article.find('a') if article.name != 'a' else article\n",
    "                    \n",
    "                    if link_tag:\n",
    "                        title = link_tag.get('title') or link_tag.text.strip()\n",
    "                        href = link_tag.get('href')\n",
    "                        \n",
    "                        # Build full URL if relative\n",
    "                        if href and not href.startswith('http'):\n",
    "                            full_link = f\"https://vietstock.vn{href}\"\n",
    "                        else:\n",
    "                            full_link = href\n",
    "                        \n",
    "                        # Create article data dictionary\n",
    "                        article_data = {\n",
    "                            'title': title,\n",
    "                            'link': full_link,\n",
    "                        }\n",
    "                        articles_data.append(article_data)\n",
    "                        page_articles_count += 1\n",
    "                        \n",
    "                        # Append to output file\n",
    "                        if output_file:\n",
    "                            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                                f.write(json.dumps(article_data, ensure_ascii=False) + '\\n')\n",
    "                \n",
    "                print(f\"  ✓ Found {page_articles_count} articles on page {page}\")\n",
    "            else:\n",
    "                print(f\"  ✗ Error calling API page {page}: {post_response.status_code}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error occurred: {e}\")\n",
    "            break\n",
    "            \n",
    "        # Delay between requests\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(f\"Completed category: {len(articles_data)} total articles\")\n",
    "    return articles_data\n",
    "\n",
    "# Output filename\n",
    "output_filename = f'vietstock_articles.jsonl'\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting crawl process\")\n",
    "print(f\"Max pages per category: {MAX_PAGE_PER_CATEGORY}\")\n",
    "print(f\"Total categories: {len(CATEGORY_CHANNEL_MAP)}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Crawl all categories\n",
    "all_articles = []\n",
    "cnt = 0\n",
    "for idx, (category_url, channel_id) in enumerate(CATEGORY_CHANNEL_MAP.items(), 1):\n",
    "    print(f\"\\n[{idx}/{len(CATEGORY_CHANNEL_MAP)}] Category: {category_url}\")\n",
    "    print(f\"Channel ID: {channel_id}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    articles = crawl_vietstock_category(category_url, channel_id, max_pages=MAX_PAGE_PER_CATEGORY, output_file=output_filename)\n",
    "    all_articles.extend(articles)\n",
    "    \n",
    "    print(f\"Total articles so far: {len(all_articles)}\")\n",
    "    time.sleep(3)  # Delay between categories\n",
    "    \n",
    "    cnt += 1\n",
    "    if LIMIT_CATEGORY:\n",
    "        if cnt >= NUM_CATEGORIES:\n",
    "            print(f\"\\n⚠ Category limit reached ({NUM_CATEGORIES}). Stopping...\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Crawling completed!\")\n",
    "print(f\"Total articles saved: {len(all_articles)}\")\n",
    "print(f\"Output file: {output_filename}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8516f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Filtering and grouping articles by VN30 stock codes...\n",
      "============================================================\n",
      "\n",
      "Total articles before filtering: 250\n",
      "Articles matching VN30 keywords: 18\n",
      "\n",
      "============================================================\n",
      "Extracting dates and summaries from article pages...\n",
      "============================================================\n",
      "\n",
      "[1/18] Extracting metadata for: VietinBank: Mục tiêu lợi nhuận trước thuế riêng lẻ...\n",
      "[2/18] Extracting metadata for: HDBank hợp tác với Trung tâm RAR - Bộ Công an: Tíc...\n",
      "[2/18] Extracting metadata for: HDBank hợp tác với Trung tâm RAR - Bộ Công an: Tíc...\n",
      "[3/18] Extracting metadata for: HDBank được Visa vinh danh 2 giải thưởng quan trọn...\n",
      "[3/18] Extracting metadata for: HDBank được Visa vinh danh 2 giải thưởng quan trọn...\n",
      "[4/18] Extracting metadata for: FMO và HDBank ký Biên bản ghi nhớ hợp tác, triển k...\n",
      "[4/18] Extracting metadata for: FMO và HDBank ký Biên bản ghi nhớ hợp tác, triển k...\n",
      "[5/18] Extracting metadata for: Ấn Độ áp thuế chống bán phá giá lên thép HRC Việt ...\n",
      "[5/18] Extracting metadata for: Ấn Độ áp thuế chống bán phá giá lên thép HRC Việt ...\n",
      "[6/18] Extracting metadata for: MB bắt tay Visa, KOTRA ra mắt thẻ doanh nghiệp đa ...\n",
      "[6/18] Extracting metadata for: MB bắt tay Visa, KOTRA ra mắt thẻ doanh nghiệp đa ...\n",
      "[7/18] Extracting metadata for: MB bắt tay Visa, Kotra ra mắt thẻ doanh nghiệp đa ...\n",
      "[7/18] Extracting metadata for: MB bắt tay Visa, Kotra ra mắt thẻ doanh nghiệp đa ...\n",
      "[8/18] Extracting metadata for: Masan Consumer: Tăng trưởng bứt phá đi cùng phát t...\n",
      "[8/18] Extracting metadata for: Masan Consumer: Tăng trưởng bứt phá đi cùng phát t...\n",
      "[9/18] Extracting metadata for: MWG sắp thực hiện 2 thương vụ chuyển nhượng công t...\n",
      "[9/18] Extracting metadata for: MWG sắp thực hiện 2 thương vụ chuyển nhượng công t...\n",
      "[10/18] Extracting metadata for: SHB được dự báo vào rổ FTSE Global All Cap: Cơ hội...\n",
      "[10/18] Extracting metadata for: SHB được dự báo vào rổ FTSE Global All Cap: Cơ hội...\n",
      "[11/18] Extracting metadata for: Mừng tuổi 32, SHB gieo hạnh phúc với ngàn ưu đãi h...\n",
      "[11/18] Extracting metadata for: Mừng tuổi 32, SHB gieo hạnh phúc với ngàn ưu đãi h...\n",
      "[12/18] Extracting metadata for: Đặc quyền SeABank tại Live concert Mỹ Tâm: Chủ thẻ...\n",
      "[12/18] Extracting metadata for: Đặc quyền SeABank tại Live concert Mỹ Tâm: Chủ thẻ...\n",
      "[13/18] Extracting metadata for: SeABank “xanh hóa” vận hành, nâng cao ý thức bảo v...\n",
      "[13/18] Extracting metadata for: SeABank “xanh hóa” vận hành, nâng cao ý thức bảo v...\n",
      "[14/18] Extracting metadata for: SeAPremium Lounge - Dấu ấn khác biệt dành cho giới...\n",
      "[14/18] Extracting metadata for: SeAPremium Lounge - Dấu ấn khác biệt dành cho giới...\n",
      "[15/18] Extracting metadata for: Sacombank ra mắt gói tài chính xanh - Hiện thực hó...\n",
      "[15/18] Extracting metadata for: Sacombank ra mắt gói tài chính xanh - Hiện thực hó...\n",
      "[16/18] Extracting metadata for: VIC dẫn dắt đà tăng, VN-Index có thêm gần 20 điểm ...\n",
      "[16/18] Extracting metadata for: VIC dẫn dắt đà tăng, VN-Index có thêm gần 20 điểm ...\n",
      "[17/18] Extracting metadata for: Cổ đông Vingroup thông qua phương án phát hành gần...\n",
      "[17/18] Extracting metadata for: Cổ đông Vingroup thông qua phương án phát hành gần...\n",
      "[18/18] Extracting metadata for: Tái định giá cổ phiếu VPB sau thương vụ IPO VPBank...\n",
      "[18/18] Extracting metadata for: Tái định giá cổ phiếu VPB sau thương vụ IPO VPBank...\n",
      "\n",
      "✓ Metadata extraction completed\n",
      "\n",
      "⚠ Date filtering is disabled (FILTER_BY_DATE=False)\n",
      "✓ Saved to 'titles_vn30_vietstock.csv'\n",
      "\n",
      "============================================================\n",
      "Statistics by stock code (after date filtering):\n",
      "============================================================\n",
      "CTG: 1 articles\n",
      "HDB: 3 articles\n",
      "HPG: 1 articles\n",
      "MBB: 2 articles\n",
      "MSN: 1 articles\n",
      "MWG: 1 articles\n",
      "SHB: 2 articles\n",
      "SSB: 3 articles\n",
      "STB: 1 articles\n",
      "VIC: 2 articles\n",
      "VPB: 1 articles\n",
      "\n",
      "============================================================\n",
      "Sample articles (first 5):\n",
      "============================================================\n",
      "\n",
      "[CTG] 2025-11-14 - VietinBank: Mục tiêu lợi nhuận trước thuế riêng lẻ 2025 đạt 32,500 tỷ, sẵn sàng gia nhập sàn vàng\n",
      "Link: https://vietstock.vn/2025/11/vietinbank-muc-tieu-loi-nhuan-truoc-thue-rieng-le-2025-dat-32500-ty-san-sang-gia-nhap-san-vang-757-1372055.htm\n",
      "Summary: Tại hội nghị nhà đầu tư được tổ chức chiều 13/11/2025, Ngân hàng TMCP Công Thương Việt Nam (VietinBa...\n",
      "\n",
      "[HDB] 2025-11-18 - HDBank hợp tác với Trung tâm RAR - Bộ Công an: Tích hợp VNeID, mang trải nghiệm số thuận tiện cho khách hàng\n",
      "Link: https://vietstock.vn/2025/11/hdbank-hop-tac-voi-trung-tam-rar-bo-cong-an-tich-hop-vneid-mang-trai-nghiem-so-thuan-tien-cho-khach-hang-757-1373083.htm\n",
      "Summary: Ngày 14/11/2025, HDBank và Trung tâm Nghiên cứu, Ứng dụng Dữ liệu Dân cư và Căn cước công dân (Trung...\n",
      "\n",
      "[HDB] 2025-11-17 - HDBank được Visa vinh danh 2 giải thưởng quan trọng\n",
      "Link: https://vietstock.vn/2025/11/hdbank-duoc-visa-vinh-danh-2-giai-thuong-quan-trong-757-1372849.htm\n",
      "Summary: Ngân hàng TMCP Phát triển TPHCM (HDBank, HOSE: HDB) vừa được Visa vinh danh với hai giải thưởng quan...\n",
      "\n",
      "[HDB] 2025-11-17 - FMO và HDBank ký Biên bản ghi nhớ hợp tác, triển khai khoản đầu tư 30 triệu USD thúc đẩy tăng trưởng xanh và bền vững tại Việt Nam\n",
      "Link: https://vietstock.vn/2025/11/fmo-va-hdbank-ky-bien-ban-ghi-nho-hop-tac-trien-khai-khoan-dau-tu-30-trieu-usd-thuc-day-tang-truong-xanh-va-ben-vung-tai-viet-nam-757-1372842.htm\n",
      "Summary: Ngày 11/11, Ngân hàng Phát triển Hà Lan (FMO) và Ngân hàng TMCP Phát triển TPHCM (HDBank, HOSE: HDB)...\n",
      "\n",
      "[HPG] 2025-11-14 - Ấn Độ áp thuế chống bán phá giá lên thép HRC Việt Nam, Hòa Phát thoát hiểm\n",
      "Link: https://vietstock.vn/2025/11/an-do-ap-thue-chong-ban-pha-gia-len-thep-hrc-viet-nam-hoa-phat-thoat-hiem-742-1372170.htm\n",
      "Summary: Bộ Tài chính Ấn Độ vừa áp dụng thuế chống bán phá giá đối với các sản phẩm thép tấm cán nóng (HRC) t...\n",
      "\n",
      "✓ Metadata extraction completed\n",
      "\n",
      "⚠ Date filtering is disabled (FILTER_BY_DATE=False)\n",
      "✓ Saved to 'titles_vn30_vietstock.csv'\n",
      "\n",
      "============================================================\n",
      "Statistics by stock code (after date filtering):\n",
      "============================================================\n",
      "CTG: 1 articles\n",
      "HDB: 3 articles\n",
      "HPG: 1 articles\n",
      "MBB: 2 articles\n",
      "MSN: 1 articles\n",
      "MWG: 1 articles\n",
      "SHB: 2 articles\n",
      "SSB: 3 articles\n",
      "STB: 1 articles\n",
      "VIC: 2 articles\n",
      "VPB: 1 articles\n",
      "\n",
      "============================================================\n",
      "Sample articles (first 5):\n",
      "============================================================\n",
      "\n",
      "[CTG] 2025-11-14 - VietinBank: Mục tiêu lợi nhuận trước thuế riêng lẻ 2025 đạt 32,500 tỷ, sẵn sàng gia nhập sàn vàng\n",
      "Link: https://vietstock.vn/2025/11/vietinbank-muc-tieu-loi-nhuan-truoc-thue-rieng-le-2025-dat-32500-ty-san-sang-gia-nhap-san-vang-757-1372055.htm\n",
      "Summary: Tại hội nghị nhà đầu tư được tổ chức chiều 13/11/2025, Ngân hàng TMCP Công Thương Việt Nam (VietinBa...\n",
      "\n",
      "[HDB] 2025-11-18 - HDBank hợp tác với Trung tâm RAR - Bộ Công an: Tích hợp VNeID, mang trải nghiệm số thuận tiện cho khách hàng\n",
      "Link: https://vietstock.vn/2025/11/hdbank-hop-tac-voi-trung-tam-rar-bo-cong-an-tich-hop-vneid-mang-trai-nghiem-so-thuan-tien-cho-khach-hang-757-1373083.htm\n",
      "Summary: Ngày 14/11/2025, HDBank và Trung tâm Nghiên cứu, Ứng dụng Dữ liệu Dân cư và Căn cước công dân (Trung...\n",
      "\n",
      "[HDB] 2025-11-17 - HDBank được Visa vinh danh 2 giải thưởng quan trọng\n",
      "Link: https://vietstock.vn/2025/11/hdbank-duoc-visa-vinh-danh-2-giai-thuong-quan-trong-757-1372849.htm\n",
      "Summary: Ngân hàng TMCP Phát triển TPHCM (HDBank, HOSE: HDB) vừa được Visa vinh danh với hai giải thưởng quan...\n",
      "\n",
      "[HDB] 2025-11-17 - FMO và HDBank ký Biên bản ghi nhớ hợp tác, triển khai khoản đầu tư 30 triệu USD thúc đẩy tăng trưởng xanh và bền vững tại Việt Nam\n",
      "Link: https://vietstock.vn/2025/11/fmo-va-hdbank-ky-bien-ban-ghi-nho-hop-tac-trien-khai-khoan-dau-tu-30-trieu-usd-thuc-day-tang-truong-xanh-va-ben-vung-tai-viet-nam-757-1372842.htm\n",
      "Summary: Ngày 11/11, Ngân hàng Phát triển Hà Lan (FMO) và Ngân hàng TMCP Phát triển TPHCM (HDBank, HOSE: HDB)...\n",
      "\n",
      "[HPG] 2025-11-14 - Ấn Độ áp thuế chống bán phá giá lên thép HRC Việt Nam, Hòa Phát thoát hiểm\n",
      "Link: https://vietstock.vn/2025/11/an-do-ap-thue-chong-ban-pha-gia-len-thep-hrc-viet-nam-hoa-phat-thoat-hiem-742-1372170.htm\n",
      "Summary: Bộ Tài chính Ấn Độ vừa áp dụng thuế chống bán phá giá đối với các sản phẩm thép tấm cán nóng (HRC) t...\n"
     ]
    }
   ],
   "source": [
    "def extract_article_metadata(article_url):\n",
    "    \"\"\"\n",
    "    Extract publication date and summary from article page's meta tags.\n",
    "    Returns tuple (date, summary) in format (YYYY-MM-DD, summary_text) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use proper browser headers to avoid 403 errors\n",
    "        metadata_headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Referer': 'https://vietstock.vn/',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Cache-Control': 'max-age=0'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(article_url, headers=metadata_headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract date from article:published_time meta tag\n",
    "        date_part = None\n",
    "        meta_date_tag = soup.find('meta', property='article:published_time')\n",
    "        if meta_date_tag and meta_date_tag.get('content'):\n",
    "            datetime_str = meta_date_tag['content']\n",
    "            date_part = datetime_str.split('T')[0]\n",
    "        \n",
    "        # Extract summary from description meta tag\n",
    "        summary = \"N/A\"\n",
    "        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "        if meta_desc_tag and meta_desc_tag.get('content'):\n",
    "            summary = meta_desc_tag['content'].strip()\n",
    "        \n",
    "        return date_part, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error extracting metadata: {e}\")\n",
    "        return None, \"N/A\"\n",
    "\n",
    "\n",
    "def is_date_in_range(date_str, start_date=START_DATE, end_date=END_DATE):\n",
    "    \"\"\"\n",
    "    Check if date string is within the specified range.\n",
    "    \"\"\"\n",
    "    if not date_str or date_str == 'N/A':\n",
    "        return False\n",
    "    try:\n",
    "        article_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        return start_date <= article_date <= end_date\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_and_group_by_codes(articles_list, keywords_map=KEYWORDS_MAP):\n",
    "    \"\"\"\n",
    "    Filter articles by VN30 keywords and group by stock codes.\n",
    "    Uses word boundaries to match full keywords only.\n",
    "    Returns a list of dictionaries with stock code and associated articles.\n",
    "    \"\"\"\n",
    "    # Dictionary to store articles for each code (using link as key to avoid duplicates)\n",
    "    code_articles = {code: {} for code in keywords_map.keys()}\n",
    "    \n",
    "    for article in articles_list:\n",
    "        # Combine title and summary for keyword matching\n",
    "        text = (article.get(\"title\", \"\") + \" \" + article.get(\"summary\", \"\")).lower()\n",
    "        matched_codes = []\n",
    "        \n",
    "        # Check each stock code\n",
    "        for code, keywords in keywords_map.items():\n",
    "            for keyword in keywords:\n",
    "                # Use word boundary regex to match full words only\n",
    "                pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n",
    "                if re.search(pattern, text):\n",
    "                    matched_codes.append(code)\n",
    "                    break\n",
    "        \n",
    "        # Add article to each matched code's dictionary (using link as key to prevent duplicates)\n",
    "        article_link = article.get('link', '')\n",
    "        for code in matched_codes:\n",
    "            if article_link and article_link not in code_articles[code]:\n",
    "                code_articles[code][article_link] = {\n",
    "                    'Stock_Code': code,\n",
    "                    'Title': article.get('title', ''),\n",
    "                    'Link': article_link,\n",
    "                    'Summary': article.get('summary', 'N/A')\n",
    "                }\n",
    "    \n",
    "    # Flatten into single list, sorted by stock code\n",
    "    result = []\n",
    "    for code in sorted(code_articles.keys()):\n",
    "        result.extend(code_articles[code].values())\n",
    "    \n",
    "    return result, {code: list(articles.values()) for code, articles in code_articles.items()}\n",
    "\n",
    "\n",
    "# Read all articles from JSONL file\n",
    "all_articles_list = []\n",
    "\n",
    "with open(output_filename, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        article = json.loads(line.strip())\n",
    "        all_articles_list.append(article)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Filtering and grouping articles by VN30 stock codes...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"Total articles before filtering: {len(all_articles_list)}\")\n",
    "\n",
    "# Filter and group by codes\n",
    "grouped_articles, code_dict = filter_and_group_by_codes(all_articles_list, keywords_map=KEYWORDS_MAP)\n",
    "\n",
    "print(f\"Articles matching VN30 keywords: {len(grouped_articles)}\")\n",
    "\n",
    "# Extract publication dates and summaries for filtered articles\n",
    "if grouped_articles:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Extracting dates and summaries from article pages...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx, article in enumerate(grouped_articles, 1):\n",
    "        print(f\"[{idx}/{len(grouped_articles)}] Extracting metadata for: {article['Title'][:50]}...\")\n",
    "        pub_date, summary = extract_article_metadata(article['Link'])\n",
    "        article['Date'] = pub_date if pub_date else 'N/A'\n",
    "        article['Summary'] = summary\n",
    "        time.sleep(0.5)  # Delay between requests\n",
    "    \n",
    "    print(\"\\n✓ Metadata extraction completed\")\n",
    "    \n",
    "    # Filter by date range\n",
    "    if FILTER_BY_DATE:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Filtering articles by date range: {START_DATE.strftime('%Y-%m-%d')} to {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        articles_before_date_filter = len(grouped_articles)\n",
    "        grouped_articles = [art for art in grouped_articles if is_date_in_range(art['Date'])]\n",
    "        \n",
    "        print(f\"Articles before date filter: {articles_before_date_filter}\")\n",
    "        print(f\"Articles after date filter: {len(grouped_articles)}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Date filtering is disabled (FILTER_BY_DATE=False)\")\n",
    "\n",
    "# Save to single CSV file\n",
    "output_csv_filename = \"titles_vn30_vietstock.csv\"\n",
    "if grouped_articles:\n",
    "    df_output = pd.DataFrame(grouped_articles)\n",
    "    # Reorder columns to have Date after Stock_Code and include Summary\n",
    "    cols = ['Stock_Code', 'Date', 'Title', 'Link', 'Summary']\n",
    "    df_output = df_output[cols]\n",
    "    df_output.to_csv(output_csv_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✓ Saved to '{output_csv_filename}'\")\n",
    "    \n",
    "    # Recalculate statistics from date-filtered articles\n",
    "    code_stats_filtered = {}\n",
    "    for article in grouped_articles:\n",
    "        code = article['Stock_Code']\n",
    "        code_stats_filtered[code] = code_stats_filtered.get(code, 0) + 1\n",
    "    \n",
    "    # Print statistics by stock code\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Statistics by stock code (after date filtering):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for code in sorted(code_stats_filtered.keys()):\n",
    "        count = code_stats_filtered[code]\n",
    "        if count > 0:\n",
    "            print(f\"{code}: {count} articles\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Sample articles (first 5):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for idx, row in df_output.head(5).iterrows():\n",
    "        print(f\"\\n[{row['Stock_Code']}] {row['Date']} - {row['Title']}\")\n",
    "        print(f\"Link: {row['Link']}\")\n",
    "        print(f\"Summary: {row['Summary'][:100]}...\" if len(str(row['Summary'])) > 100 else f\"Summary: {row['Summary']}\")\n",
    "else:\n",
    "    print(\"No articles matched VN30 keywords or date range!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "684316b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"./vietstock_articles.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vn30_crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
